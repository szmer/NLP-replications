{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download and unpack `sentence polarity dataset v1.0` from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "2. Download BNC (TODO)\n",
    "3. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'non-proximal', '0371', 'Socialist-leaning', 'b.', 'Nahdatul', 'Gandhian', 'fishpools', 'cloudiness', '1.1-litre']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\NP 0 2> (<L (S[dcl]\\NP)/NP POS POS chases (S[dcl]\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\NP 0 2> (<L (NP\\NP)/NP POS POS of (NP\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will run the underlying parser with pexpect, and intercept its outputs from within Python\n",
    "import pexpect\n",
    "parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "parser.expect('Model loaded, ready to parse.')\n",
    "parser.send('The cat chases a ball of yarn.\\n')\n",
    "parser.expect('ID')\n",
    "parser.expect('\\n\\(.*\\n')\n",
    "parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "print(parser_output)\n",
    "parser.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T_S[dcl]_1_2>\n",
      "  (<T_NP[nb]_0_2> (The ) (cat ))\n",
      "  (<T_S[dcl]\\NP_0_2>\n",
      "    (chases )\n",
      "    (<T_NP[nb]_0_2>\n",
      "      (<T_NP[nb]_0_2> (a ) (ball ))\n",
      "      (<T_NP\\NP_0_2> (of ) (<T_NP_0_1> (yarn. ))))))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "only_word = re.compile(r'<L\\s\\S+\\sPOS\\sPOS\\s(\\S+)\\s\\S+>')\n",
    "concat_label = re.compile(r'<(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)>')\n",
    "\n",
    "# some string cleanup\n",
    "def clean_parser_output(parse_output):\n",
    "    return concat_label.sub(lambda match: '<'+match.group(1)+'_'+match.group(2).replace('(', '[').replace(')', ']')\n",
    "                            +'_'+match.group(3)+'_'+match.group(4)+'>',\n",
    "                            only_word.sub(lambda match: match.group(1), parse_output))\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parser_output(parser_output))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `(parenthesized expression)`, the first item `(head)` is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = { word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sentences from the corpus, with words as their indices in the word map.\n",
    "corpus_sents = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            corpus_sents.append([])\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_sents[-1].append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "vocabulary_size = len(unique_words) + 1 # add the boundary token\n",
    "embedding_size = 128\n",
    "batch_size = 128\n",
    "# Number of sample correct word pairs to be shown to word2vec for one random target word.\n",
    "num_samples = 4\n",
    "assert num_samples % 2 == 0\n",
    "assert batch_size % num_samples == 0\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vocabulary_size - 1\n",
    "\n",
    "corp_runs = 1\n",
    "sent_step = 1 # we train 1/sent_step of all the sentences\n",
    "\n",
    "def skipgram_batches():\n",
    "    for run_n in range(corp_runs):\n",
    "        sent_n = 0\n",
    "        word_n = 0\n",
    "        \n",
    "        target_n = 0 # relative to the current batch\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        while sent_n < len(corpus_sents):\n",
    "            for j in range(num_samples):\n",
    "                batch[target_n*num_samples+j] = corpus_sents[sent_n][word_n]\n",
    "            # \"Good\" examples - words near the target (we will let TensorFlow randomize the \"bad\" ones)\n",
    "            for j in range(num_samples // 2):\n",
    "                labels[target_n*num_samples+j*2][0] = (corpus_sents[sent_n][word_n-j-1] if word_n-j-1 >= 0\n",
    "                                                       else bound_token_id)\n",
    "                labels[target_n*num_samples+j*2+1][0] = (corpus_sents[sent_n][word_n+j+1]\n",
    "                                                         if word_n+j+1 < len(corpus_sents[sent_n])\n",
    "                                                         else bound_token_id)\n",
    "                \n",
    "            target_n += 1\n",
    "            if target_n == (batch_size // num_samples):\n",
    "                yield batch, labels, False\n",
    "                batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "                target_n = 0\n",
    "                \n",
    "            word_n += 1\n",
    "            try:\n",
    "                while word_n == len(corpus_sents[sent_n]):\n",
    "                    word_n = 0\n",
    "                    sent_n += sent_step\n",
    "                    if (floor(sent_n / len(corpus_sents) * 10)\n",
    "                        > floor((sent_n-sent_step) / len(corpus_sents) * 10)):\n",
    "                        print('{}0%'.format(floor(sent_n / len(corpus_sents) * 10)), end=' ')\n",
    "            except IndexError: # happens on the end of the corpus\n",
    "                break\n",
    "                \n",
    "        batch[target_n:] = 0.0\n",
    "        labels[target_n:, :] = 0.0\n",
    "        yield batch, labels, (run_n == corp_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    # Model parameters: word embeddings and model weights & biases for each word.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # The computation graph.\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "    \n",
    "    # Number of random words to sample apart from the true target; the model should learn to\n",
    "    # assign low probability to them given the context.\n",
    "    negative_samples_n = batch_size\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=labels,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         num_sampled=negative_samples_n,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2017-12-07 00:00:08.931667\n",
      "(loss: 694.7510986328125) (loss: 0.8954055905342102) 10% (loss: 13.901857376098633) 20% (loss: 1.1992886066436768) (loss: 7.327809810638428) 30% (loss: 0.809363842010498) 40% (loss: 3.329014778137207) 50% (loss: 2.245791435241699) 60% (loss: 98.71966552734375) (loss: 0.8352644443511963) 70% (loss: 0.26813966035842896) 80% (loss: 0.46178850531578064) 90% (loss: 0.6541154384613037) 100% Final loss: 12.5188\n",
      "Training end: 2017-12-07 01:06:41.422379\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    print('Training start:', datetime.datetime.now())\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    for batch_inputs, batch_labels, is_last in skipgram_batches():\n",
    "        if is_last:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "            print('Training end:', datetime.datetime.now())\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            if (i % 250000 == 0):\n",
    "                print('(loss: {})'.format(loss_val), end=' ')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(word):\n",
    "    dists = np.abs(trained_embeddings - trained_embeddings[word_map[word], ]).sum(axis=1)\n",
    "    dists[word_map[word]] = 1e6\n",
    "    return unique_words[dists.argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_cos_neighbor(word):\n",
    "    dists = (np.dot(trained_embeddings, trained_embeddings[word_map[word],])\n",
    "             / np.linalg.norm(trained_embeddings) * np.linalg.norm(trained_embeddings[word_map[word],]))\n",
    "    dists[word_map[word]] = 1e6\n",
    "    return unique_words[dists.argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest word vectors for:\n",
      "cat: name\n",
      "doctor: report\n",
      "cold: light\n",
      "blue: black\n",
      "red: white\n",
      "walk: run\n",
      "bring: put\n",
      "is: was\n",
      "Europe: RNav\n"
     ]
    }
   ],
   "source": [
    "print('Nearest word vectors for:')\n",
    "print('cat:', nearest_neighbor('cat'))\n",
    "print('doctor:', nearest_neighbor('doctor'))\n",
    "print('cold:', nearest_neighbor('cold'))\n",
    "print('blue:', nearest_neighbor('blue'))\n",
    "print('red:', nearest_neighbor('red'))\n",
    "print('walk:', nearest_neighbor('walk'))\n",
    "print('bring:', nearest_neighbor('bring'))\n",
    "print('is:', nearest_neighbor('is'))\n",
    "print('Europe:', nearest_neighbor('europe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest word vectors for:\n",
      "cat: Ivory&Sime\n",
      "doctor: £30,952\n",
      "cold: $2.4billion\n",
      "blue: £630million\n",
      "red: collapse'\n",
      "walk: £60,837\n",
      "bring: AWARRANT\n",
      "is: AWARRANT\n",
      "Europe: McCran\n"
     ]
    }
   ],
   "source": [
    "print('Nearest word vectors for:')\n",
    "print('cat:', nearest_cos_neighbor('cat'))\n",
    "print('doctor:', nearest_cos_neighbor('doctor'))\n",
    "print('cold:', nearest_cos_neighbor('cold'))\n",
    "print('blue:', nearest_cos_neighbor('blue'))\n",
    "print('red:', nearest_cos_neighbor('red'))\n",
    "print('walk:', nearest_cos_neighbor('walk'))\n",
    "print('bring:', nearest_cos_neighbor('bring'))\n",
    "print('is:', nearest_cos_neighbor('is'))\n",
    "print('Europe:', nearest_cos_neighbor('europe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_W = Variable(torch.randn(embedding_size*2, embedding_size), requires_grad=True)\n",
    "enc_b = Variable(torch.zeros(1, embedding_size), requires_grad=True)\n",
    "dec_W = Variable(torch.randn(embedding_size, embedding_size*2), requires_grad=True)\n",
    "dec_b = Variable(torch.zeros(1, embedding_size*2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_node(child1, child2):\n",
    "    #\"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size). Return the encoding\n",
    "    #(1, embedding_size).\"\"\"\n",
    "    conc_embeds = Variable(torch.cat((child1.data, child2.data), 0))\n",
    "    # we use.view() because we need to make sure that the return value is a vector (as word embeddings),\n",
    "    # not a matrix\n",
    "    return conc_embeds.matmul(enc_W).add(enc_b).tanh().view(embedding_size)\n",
    "\n",
    "def decode_node(node):\n",
    "    # node is (1, embedding_size), output is (1, 2*embedding_size)\n",
    "    return node.matmul(dec_W).add(dec_b).tanh().view(embedding_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from random import choice, randint\n",
    "encoding_train_batch_size = 50 # number of sentences\n",
    "\n",
    "# Handle special treatment of parens by our parser.\n",
    "def nd_lbl(node):\n",
    "    if node.label() == '-LRB-':\n",
    "        return '('\n",
    "    elif node.label() == '-RRB-':\n",
    "        return ')'\n",
    "    else:\n",
    "        return node.label()\n",
    "\n",
    "# Note that node_encodings are passed by value, so we always modify the dictionary given to\n",
    "# the topmost function call.\n",
    "def encode_tree(node, node_encodings):\n",
    "    \"Encode_tree returns a pair of lists of partial derivatives for encoding matrix and bias\"\n",
    "    subtrees = [subtr for subtr in node]\n",
    "    if len(subtrees) == 0: # a leaf\n",
    "        if nd_lbl(node) in word_map:\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[word_map[nd_lbl(node)], ]))\n",
    "        else: # replace unknowns with a random word\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[randint(0, trained_embeddings.shape[0]), ]))\n",
    "    elif len(subtrees) == 1:\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = node_encodings[nd_lbl(subtrees[0])]\n",
    "    else:\n",
    "        if len(subtrees) != 2: # dbg\n",
    "            print(subtrees)\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        encode_tree(subtrees[1], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = encode_node(\n",
    "            node_encodings[nd_lbl(subtrees[0])],\n",
    "            node_encodings[nd_lbl(subtrees[1])])\n",
    "\n",
    "def make_parser():\n",
    "    parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "    parser.expect('Model loaded, ready to parse.')\n",
    "    return parser\n",
    "\n",
    "def kill_parser(parser):\n",
    "    parser.terminate()\n",
    "    \n",
    "def sentence_tree(sentence_form, parser):\n",
    "    parser.send(sentence_form+'\\n')\n",
    "    # (this secures us from finding one of the patterns below in the sentence itself:)\n",
    "    response = parser.expect([pexpect.TIMEOUT, 'ID', pexpect.EOF])\n",
    "    if response == 1: # can't happen if timed out\n",
    "        response = parser.expect(['Skipping sentence of length', '\\n\\(.*\\n', pexpect.TIMEOUT])\n",
    "    if response in [0, 2]:\n",
    "        return False\n",
    "    parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "    return ParentedTree.fromstring(clean_parser_output(parser_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 error:  80.32785034179688\n",
      "Batch 2 error:  87.45626831054688\n",
      "Batch 3 error:  79.1351318359375\n",
      "Batch 4 error:  51.07670974731445\n",
      "Batch 5 error:  61.075645446777344\n",
      "Batch 6 error:  52.87057113647461\n",
      "Batch 7 error:  37.8617057800293\n",
      "Batch 8 error:  64.2613754272461\n",
      "Batch 9 error:  45.33639144897461\n",
      "Batch 10 error:  45.88460922241211\n",
      "Batch 11 error:  25.048158645629883\n",
      "Batch 12 error:  22.817052841186523\n",
      "Batch 13 error:  18.430747985839844\n",
      "Batch 14 error:  20.823261260986328\n",
      "Batch 15 error:  16.265684127807617\n",
      "Batch 16 error:  19.840744018554688\n",
      "Batch 17 error:  10.855727195739746\n",
      "Batch 18 error:  14.807278633117676\n",
      "Batch 19 error:  17.29961395263672\n",
      "Batch 20 error:  15.816706657409668\n",
      "Batch 21 error:  4.427694797515869\n",
      "Batch 22 error:  6.532020092010498\n",
      "Batch 23 error:  21.105663299560547\n",
      "Batch 24 error:  7.627029895782471\n",
      "Batch 25 error:  4.137449264526367\n"
     ]
    }
   ],
   "source": [
    "iters_n = 25\n",
    "encoding_train_batch_size = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iter_i in range(iters_n):\n",
    "    used_sents = [] # at least don't repeat them in one batch\n",
    "    batch_accum_error = 0\n",
    "    parser = make_parser()\n",
    "    nodes_n = 0 # count them to average the error\n",
    "    \n",
    "    for i in range(encoding_train_batch_size):\n",
    "        tree = False\n",
    "        # It's possible that sentence_tree() returns False, if the sentence was too long and\n",
    "        # rejected by the parser, or it timeouts.\n",
    "        while not tree:\n",
    "            sentence_n = randint(0, len(corpus_sents))\n",
    "            while sentence_n in used_sents:\n",
    "                sentence_n = randint(0, len(corpus_sents))\n",
    "            sentence = corpus_sents[sentence_n]\n",
    "            used_sents.append(sentence_n)\n",
    "            \n",
    "            sentence_form = ' '.join([unique_words[word_id] for word_id in sentence])\n",
    "            #print(sentence_n, sentence_form)\n",
    "            tree = sentence_tree(sentence_form, parser)\n",
    "\n",
    "        # Encode the tree.\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree, node_encodings)\n",
    "        \n",
    "        # Decode the tree back again.\n",
    "        # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "        # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "        node_decodings = dict()\n",
    "        node_decodings[nd_lbl(tree.root())] = node_encodings[nd_lbl(tree.root())]\n",
    "        nodes_to_visit = [ tree.root() ]\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            children = [child for child in current_node]\n",
    "            if len(children) == 0:\n",
    "                continue\n",
    "            elif len(children) == 2: # not a leaf\n",
    "                decoded_node = decode_node(node_decodings[nd_lbl(current_node)])\n",
    "                node_decodings[nd_lbl(children[0])] = decoded_node[:embedding_size]\n",
    "                node_decodings[nd_lbl(children[1])] = decoded_node[embedding_size:]\n",
    "                \n",
    "                #print(node_encodings[nd_lbl(current_node)])\n",
    "                #print(node_decodings[nd_lbl(current_node)])\n",
    "                err = node_encodings[nd_lbl(current_node)].sub(node_decodings[nd_lbl(current_node)]).abs().sum()\n",
    "                err.backward() # accumulate gradient\n",
    "                batch_accum_error += err.data\n",
    "                nodes_n += 1\n",
    "            else:\n",
    "                raise RuntimeError('unexpected number of node children in decode:' + str(children))\n",
    "        \n",
    "    kill_parser(parser)\n",
    "    print('Batch', iter_i+1, 'error: ', (batch_accum_error / nodes_n)[0])\n",
    "    if batch_accum_error[0] == 0:\n",
    "        for sentence_n in used_sents:\n",
    "            print(' '.join([unique_words[word_id] for word_id in corpus_sents[sentence_n]]))\n",
    "        raise RuntimeError\n",
    "    enc_W.data -= enc_W.grad.data * learning_rate\n",
    "    enc_b.data -= enc_b.grad.data * learning_rate\n",
    "    dec_W.data -= dec_W.grad.data * learning_rate\n",
    "    dec_b.data -= dec_b.grad.data * learning_rate\n",
    "    #print('ENC_W', enc_W.grad, 'ENC_B', enc_b.grad, 'DEC_W', dec_W.grad, 'DEC_B', dec_b.grad)\n",
    "    \n",
    "    enc_W.grad.data.zero_()\n",
    "    enc_b.grad.data.zero_()\n",
    "    dec_W.grad.data.zero_()\n",
    "    dec_b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/szymon/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentence_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', \"century's\", 'new', '\"', 'conan', '\"', 'and', 'that', \"he's\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co-writer/director', 'peter', \"jackson's\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', \"tolkien's\", 'middle-earth', '.'], ['effective', 'but', 'too-tepid', 'biopic']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "print(sentence_polarity.sents(categories='pos')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the sentence polarity corpus into test and training slices in proportion 10/90, just as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "assert len(sentence_polarity.sents(categories='pos')) == len(sentence_polarity.sents(categories='neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing start: 2017-12-05 23:48:20.724207\n",
      "Parsing end: 2017-12-06 00:57:26.937319\n"
     ]
    }
   ],
   "source": [
    "train_pnt, test_pnt = 0, 0\n",
    "test_corp_len = (len(sentence_polarity.sents(categories='pos')) // 10\n",
    "                 + len(sentence_polarity.sents(categories='neg')) // 10)\n",
    "sent_pol_len = len(sentence_polarity.sents())\n",
    "\n",
    "train_sent_vecs = np.zeros((sent_pol_len - test_corp_len, embedding_size))\n",
    "test_sent_vecs = np.zeros((test_corp_len, embedding_size))\n",
    "train_sent_labels = np.zeros((sent_pol_len - test_corp_len, 1))\n",
    "test_sent_labels = np.zeros((test_corp_len, 1))\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "print('Parsing start:', datetime.datetime.now())\n",
    "for (label, sents) in [(1.0, sentence_polarity.sents(categories='pos')),\n",
    "                      (0.0, sentence_polarity.sents(categories='neg'))]:\n",
    "    sents = list(sents)\n",
    "    test_ids = sample(range(len(sents)), len(sents) // 10)\n",
    "    for sent_i in range(len(sents)):\n",
    "        tree = sentence_tree(' '.join(sents[sent_i]), parser)\n",
    "        if not tree: # sentence too long, or times out the parser\n",
    "            continue\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree, node_encodings)\n",
    "        if sent_i in test_ids:\n",
    "            #print(node_encodings[nd_lbl(tree.root())])\n",
    "            test_sent_vecs[test_pnt, :] = node_encodings[nd_lbl(tree.root())].data.numpy()\n",
    "            test_sent_labels[test_pnt, 0] = label\n",
    "            test_pnt += 1\n",
    "        else:\n",
    "            #print(node_encodings[nd_lbl(tree.root())])\n",
    "            train_sent_vecs[train_pnt, :] = node_encodings[nd_lbl(tree.root())].data.numpy()\n",
    "            train_sent_labels[train_pnt, 0] = label\n",
    "            train_pnt += 1\n",
    "kill_parser(parser)\n",
    "print('Parsing end:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper seems to use just a binary classifier of sentence vectors, without any neural net hidden layers. It is the approach we will try first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "regr_weights = tf.Variable(tf.random_normal([embedding_size, 1]))\n",
    "regr_bias = tf.Variable(tf.random_normal([1, 1]))\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, embedding_size], name='samples')\n",
    "Y = tf.placeholder(\"float\", [None, 1], name='labels')\n",
    "\n",
    "prediction = tf.sigmoid(tf.matmul(X, regr_weights) + regr_bias)\n",
    "loss = tf.losses.sigmoid_cross_entropy(prediction, Y)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.5).minimize(loss)\n",
    "\n",
    "acc = tf.metrics.accuracy(tf.round(prediction), tf.round(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20001\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer()) # needed for accuracy metric\n",
    "    for epoch_i in range(epochs):\n",
    "        _, curr_loss = sess.run([optimizer, loss],\n",
    "                                feed_dict={X: train_sent_vecs,\n",
    "                                           Y: train_sent_labels})\n",
    "        if epoch_i % 1000 == 0:\n",
    "            print(epoch_i, curr_loss)\n",
    "    \n",
    "    _, test_acc = sess.run([loss, acc], feed_dict={X: test_sent_vecs,\n",
    "                                          Y: test_sent_labels})\n",
    "    print('Test accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Try to do an autoencoder without loops: https://groups.google.com/forum/#!topic/theano-users/O5CM49-jMqQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
