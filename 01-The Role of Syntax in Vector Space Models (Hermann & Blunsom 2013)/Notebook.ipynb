{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download and unpack `sentence polarity dataset v1.0` from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "2. Download BNC (TODO)\n",
    "3. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'yuh', 'MINES', 'Rosaldo', 'UPJOHN', 'Ming/Qing', 'binned', '14-year-old', 'froings', 'not-so-prodigal']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\NP 0 2> (<L (S[dcl]\\NP)/NP POS POS chases (S[dcl]\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\NP 0 2> (<L (NP\\NP)/NP POS POS of (NP\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will run the underlying parser with pexpect, and intercept its outputs from within Python\n",
    "import pexpect\n",
    "parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "parser.expect('Model loaded, ready to parse.')\n",
    "parser.send('The cat chases a ball of yarn.\\n')\n",
    "parser.expect('ID')\n",
    "parser.expect('\\n\\(.*\\n')\n",
    "parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "print(parser_output)\n",
    "parser.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T_S[dcl]_1_2>\n",
      "  (<T_NP[nb]_0_2> (The ) (cat ))\n",
      "  (<T_S[dcl]\\NP_0_2>\n",
      "    (chases )\n",
      "    (<T_NP[nb]_0_2>\n",
      "      (<T_NP[nb]_0_2> (a ) (ball ))\n",
      "      (<T_NP\\NP_0_2> (of ) (<T_NP_0_1> (yarn. ))))))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "only_word = re.compile(r'<L\\s\\S+\\sPOS\\sPOS\\s(\\S+)\\s\\S+>')\n",
    "concat_label = re.compile(r'<(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)>')\n",
    "\n",
    "# some string cleanup\n",
    "def clean_parser_output(parse_output):\n",
    "    return concat_label.sub(lambda match: '<'+match.group(1)+'_'+match.group(2).replace('(', '[').replace(')', ']')\n",
    "                            +'_'+match.group(3)+'_'+match.group(4)+'>',\n",
    "                            only_word.sub(lambda match: match.group(1), parse_output))\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parser_output(parser_output))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `(parenthesized expression)`, the first item `(head)` is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = { word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sentences from the corpus, with words as their indices in the word map.\n",
    "corpus_sents = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            corpus_sents.append([])\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_sents[-1].append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Corpus_iter:\n",
    "#    def __iter__(self):\n",
    "#        for sent in corpus_sents:\n",
    "#            yield [unique_words[wi] for wi in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corp_iter = Corpus_iter()\n",
    "#w2v_model = gensim.models.Word2Vec(corp_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE\n",
    "\n",
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "vocabulary_size = len(unique_words) + 1 # add the boundary token\n",
    "embedding_size = 70\n",
    "batch_size = 128\n",
    "# Number of sample correct word pairs to be shown to word2vec for one random target word.\n",
    "num_samples = 16\n",
    "assert num_samples % 2 == 0\n",
    "assert batch_size % num_samples == 0\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vocabulary_size - 1\n",
    "\n",
    "corp_runs = 10\n",
    "sent_step = 1\n",
    "\n",
    "def skipgram_batches():\n",
    "    for run_n in range(corp_runs):\n",
    "        sent_n = 0\n",
    "        word_n = 0\n",
    "        \n",
    "        target_n = 0 # relative to the current batch\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        while sent_n < len(corpus_sents):\n",
    "            for j in range(num_samples):\n",
    "                labels[target_n*num_samples+j][0] = corpus_sents[sent_n][word_n]\n",
    "            for j in range(num_samples // 2):\n",
    "                batch[target_n*num_samples+j*2] = (corpus_sents[sent_n][word_n-j] if word_n-j >= 0\n",
    "                                                   else bound_token_id)\n",
    "                batch[target_n*num_samples+j*2+1] = (corpus_sents[sent_n][word_n+j]\n",
    "                                                     if word_n+j < len(corpus_sents[sent_n])\n",
    "                                                     else bound_token_id)\n",
    "                \n",
    "            target_n += 1\n",
    "            if target_n == (batch_size // num_samples):\n",
    "                yield batch, labels, False\n",
    "                batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "                target_n = 0\n",
    "                \n",
    "            word_n += 1\n",
    "            try:\n",
    "                while word_n == len(corpus_sents[sent_n]):\n",
    "                    word_n = 0\n",
    "                    sent_n += sent_step\n",
    "                    if (floor(sent_n / len(corpus_sents) * 10)\n",
    "                        > floor((sent_n-sent_step) / len(corpus_sents) * 10)):\n",
    "                        print('{}0%'.format(floor(sent_n / len(corpus_sents) * 10)), end=' ')\n",
    "            except IndexError: # happens on the end of the corpus\n",
    "                break\n",
    "                \n",
    "        batch[target_n:] = 0.0\n",
    "        labels[target_n:, :] = 0.0\n",
    "        yield batch, labels, (run_n == corp_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    # Model parameters: word embeddings and model weights & biases for each word.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REMOVE\n",
    "##with tf.device('/cpu:0'):\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # The computation graph.\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "    \n",
    "    # Number of random words to sample apart from the true target; the model should learn to\n",
    "    # assign low probability to them given the context.\n",
    "    negative_samples_n = batch_size * 2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=labels,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         num_sampled=negative_samples_n,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    optimizer = tf.train.AdagradOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2017-11-27 00:55:43.982109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loss: 1339.1641845703125) (loss: 90.64764404296875) (loss: 0.3690961003303528) (loss: 1.606388807296753) (loss: 33.17181396484375) (loss: 4.64590311050415) 10% (loss: 36.2855224609375) (loss: 1.1286957263946533) (loss: 32.92365646362305) (loss: 71.0347900390625) (loss: 0.2758979797363281) 20% (loss: 4.5707807540893555) (loss: 4.296047687530518) (loss: 0.7888756990432739) (loss: 3.196495532989502) (loss: 46.637062072753906) (loss: 44.94758605957031) 30% (loss: 0.2709590494632721) (loss: 1.7694886922836304) (loss: 15.963029861450195) (loss: 6.522884368896484) (loss: 1.9111801385879517) 40% (loss: 8.41146183013916) (loss: 2.555312156677246) (loss: 0.2825905382633209) (loss: 0.7162767648696899) (loss: 0.1939745843410492) (loss: 7.535548210144043) 50% (loss: 0.24661734700202942) (loss: 3.205333948135376) (loss: 2.1553921699523926) (loss: 0.8445965051651001) 60% (loss: 257.6120910644531) (loss: 0.7380498647689819) (loss: 2.093625545501709) (loss: 13.904006958007812) (loss: 1.5279934406280518) (loss: 1.4681955575942993) 70% (loss: 0.8285261392593384) (loss: 1.685684084892273) (loss: 0.7538851499557495) (loss: 1.505164384841919) (loss: 1.4065330028533936) 80% (loss: 0.344400554895401) (loss: 0.06215371936559677) 90% (loss: 41.95425796508789) (loss: 1.0060170888900757) (loss: 5.87442684173584) (loss: 9.800907135009766) (loss: 1.861735224723816) 100% (loss: 1.4200687408447266) (loss: 0.5297969579696655) (loss: 3.0216755867004395) (loss: 1.1296577453613281) (loss: 25.33552360534668) 10% (loss: 2.092766046524048) (loss: 0.44211065769195557) (loss: 1.3143948316574097) (loss: 1.5712637901306152) (loss: 1.6746294498443604) 20% (loss: 18.910194396972656) (loss: 3.0947089195251465) (loss: 3.1164793968200684) (loss: 0.2916925251483917) (loss: 0.6109145283699036) (loss: 0.8113582730293274) 30% (loss: 0.5248873829841614) (loss: 3.160712718963623) (loss: 0.3491382300853729) (loss: 1.4876145124435425) (loss: 2.607659339904785) 40% (loss: 1.63040030002594) (loss: 0.6703082323074341) (loss: 14.061260223388672) (loss: 2.0840578079223633) (loss: 0.3701634109020233) (loss: 1.0481152534484863) 50% (loss: 1.3418140411376953) (loss: 0.7821566462516785) (loss: 1.5173192024230957) (loss: 0.7046140432357788) 60% (loss: 7.926731586456299) (loss: 15.647209167480469) (loss: 0.8748239278793335) (loss: 4.525722503662109) (loss: 0.526386022567749) (loss: 5.26846170425415) 70% (loss: 0.5205659866333008) (loss: 0.4854713976383209) (loss: 0.43535107374191284) (loss: 6.203177452087402) (loss: 0.624253511428833) 80% (loss: 0.27642491459846497) (loss: 0.21006886661052704) 90% (loss: 0.33124077320098877) (loss: 0.28271040320396423) (loss: 16.253726959228516) (loss: 1.7674740552902222) (loss: 7.841939926147461) 100% (loss: 0.7879422903060913) (loss: 1.4754797220230103) (loss: 1.2028814554214478) (loss: 0.9645442962646484) (loss: 0.9557801485061646) 10% (loss: 1.936352252960205) (loss: 2.015885829925537) (loss: 2.532533645629883) (loss: 0.16095180809497833) (loss: 0.4379431903362274) 20% (loss: 0.4435395300388336) (loss: 53.01224136352539) (loss: 0.511820912361145) (loss: 0.7229578495025635) (loss: 1.1720459461212158) (loss: 1.5641860961914062) 30% (loss: 0.6892101764678955) (loss: 28.517047882080078) (loss: 0.8540616631507874) (loss: 4.621870040893555) (loss: 1.05057954788208) 40% (loss: 1.3568429946899414) (loss: 1.8624694347381592) (loss: 1.514846682548523) (loss: 0.5267354846000671) (loss: 0.3928193747997284) (loss: 1.497605562210083) 50% (loss: 0.5116371512413025) (loss: 0.2681021988391876) (loss: 1.212829351425171) (loss: 5.313158988952637) 60% (loss: 4.933629035949707) (loss: 2.66290283203125) (loss: 0.14600379765033722) (loss: 1.0004054307937622) (loss: 0.12267294526100159) (loss: 1.1739399433135986) 70% (loss: 1.405626893043518) (loss: 1.4551031589508057) (loss: 0.41578301787376404) (loss: 0.9979717135429382) (loss: 3.1974267959594727) 80% (loss: 0.8394929766654968) (loss: 0.2487693428993225) 90% (loss: 0.1725362241268158) (loss: 0.33432430028915405) (loss: 0.7980184555053711) (loss: 0.732602596282959) (loss: 1.129701852798462) 100% (loss: 7.503744602203369) (loss: 2.400066614151001) (loss: 0.46684467792510986) (loss: 0.7577917575836182) (loss: 1.1913970708847046) 10% (loss: 0.5301326513290405) (loss: 0.7964338064193726) (loss: 1.194749355316162) (loss: 0.8416458368301392) (loss: 0.5556988716125488) (loss: 1.0766371488571167) 20% (loss: 0.909804105758667) (loss: 4.255663871765137) (loss: 1.0829886198043823) (loss: 5.707108020782471) (loss: 1.1540117263793945) 30% (loss: 0.9761175513267517) (loss: 7.1532182693481445) (loss: 1.5786025524139404) (loss: 0.43885141611099243) (loss: 0.6550471186637878) 40% (loss: 0.47629767656326294) (loss: 0.81345134973526) (loss: 0.2234366238117218) (loss: 0.9232571125030518) (loss: 0.9953917264938354) (loss: 0.8622602820396423) 50% (loss: 0.06670010089874268) (loss: 4.247426986694336) (loss: 0.9504410028457642) (loss: 0.42367327213287354) 60% (loss: 1.8718007802963257) (loss: 1.1506671905517578) (loss: 2.0292229652404785) (loss: 1.415603518486023) (loss: 1.0155683755874634) (loss: 2.2206315994262695) 70% (loss: 1.8613696098327637) (loss: 1.1580162048339844) (loss: 2.2059476375579834) (loss: 0.23148930072784424) (loss: 1.6769835948944092) 80% (loss: 0.8424974679946899) (loss: 1.8837249279022217) 90% (loss: 0.8831688165664673) (loss: 1.0293769836425781) (loss: 0.9938175678253174) (loss: 0.7947710752487183) (loss: 0.5288528203964233) 100% (loss: 0.5080244541168213) (loss: 1.1361191272735596) (loss: 1.0550520420074463) (loss: 0.9745680093765259) (loss: 0.22868703305721283) 10% (loss: 0.4987291991710663) (loss: 0.8503929376602173) (loss: 2.3096060752868652) (loss: 0.16249611973762512) (loss: 10.624580383300781) (loss: 3.0413103103637695) 20% (loss: 0.5250838994979858) (loss: 0.3487935960292816) (loss: 0.49378839135169983) (loss: 2.2993178367614746) (loss: 0.7372597455978394) 30% (loss: 1.1768733263015747) (loss: 1.6923962831497192) (loss: 1.0830647945404053) (loss: 0.42573082447052) (loss: 14.155308723449707) 40% (loss: 1.1381475925445557) (loss: 0.41040247678756714) (loss: 2.7786757946014404) (loss: 0.9056270122528076) (loss: 3.3687148094177246) (loss: 3.9677975177764893) 50% (loss: 10.670615196228027) (loss: 1.0911731719970703) (loss: 0.44861385226249695) (loss: 0.6319419145584106) 60% (loss: 5.012646675109863) (loss: 0.5076494216918945) (loss: 1.2995587587356567) (loss: 0.661583662033081) (loss: 0.0845855325460434) (loss: 1.1536649465560913) 70% (loss: 0.5870996713638306) (loss: 0.6719793081283569) (loss: 0.7548373341560364) (loss: 0.8329620957374573) (loss: 1.296557903289795) 80% (loss: 1.2144525051116943) (loss: 0.1899908185005188) 90% (loss: 1.0224891901016235) (loss: 0.9604949951171875) (loss: 0.8148503303527832) (loss: 1.5938899517059326) (loss: 1.1224663257598877) 100% (loss: 0.8751121163368225) (loss: 0.38391733169555664) (loss: 0.8844602704048157) (loss: 0.9844093918800354) (loss: 0.6337645649909973) 10% (loss: 0.5169445276260376) (loss: 2.455526828765869) (loss: 0.5832750201225281) (loss: 0.7289268970489502) (loss: 1.2686445713043213) (loss: 0.9817907214164734) 20% (loss: 1.016322374343872) (loss: 0.7730216383934021) (loss: 6.326076507568359) (loss: 0.1444876492023468) (loss: 1.4112088680267334) 30% (loss: 1.5692718029022217) (loss: 0.5626704692840576) (loss: 0.28320980072021484) (loss: 0.9990135431289673) (loss: 1.3985085487365723) 40% (loss: 1.2003270387649536) (loss: 0.5301978588104248) (loss: 1.1497273445129395) (loss: 0.5846965909004211) (loss: 0.8234107494354248) (loss: 0.7338545918464661) 50% (loss: 4.87510871887207) (loss: 1.7853158712387085) (loss: 0.28535425662994385) (loss: 0.7795819044113159) (loss: 0.39958426356315613) 60% (loss: 3.8603456020355225) (loss: 7.2560715675354) (loss: 0.823948860168457) (loss: 9.160748481750488) (loss: 2.493251323699951) 70% (loss: 0.16163843870162964) (loss: 0.8264720439910889) (loss: 0.12730969488620758) (loss: 0.8605630397796631) (loss: 0.745621383190155) 80% (loss: 1.580132007598877) (loss: 0.22607295215129852) 90% (loss: 0.6122738122940063) (loss: 5.532595634460449) (loss: 0.5072629451751709) (loss: 0.4325673580169678) (loss: 1.4982892274856567) 100% (loss: 1.5173499584197998) (loss: 0.5949144959449768) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loss: 0.8147010803222656) (loss: 2.940011978149414) (loss: 2.010826349258423) 10% (loss: 4.737893581390381) (loss: 2.0707268714904785) (loss: 0.30549025535583496) (loss: 1.4315439462661743) (loss: 2.1357955932617188) (loss: 3.7092084884643555) 20% (loss: 1.5353803634643555) (loss: 0.7573847770690918) (loss: 0.7351540923118591) (loss: 0.18051570653915405) (loss: 0.8986968994140625) 30% (loss: 0.9448647499084473) (loss: 0.6661956906318665) (loss: 1.36269211769104) (loss: 0.8184607625007629) (loss: 6.10341215133667) (loss: 2.1489791870117188) 40% (loss: 3.8815298080444336) (loss: 1.1165080070495605) (loss: 1.6206552982330322) (loss: 0.706818699836731) (loss: 1.1072916984558105) 50% (loss: 6.321184158325195) (loss: 0.9157577753067017) (loss: 0.3018455505371094) (loss: 0.6180638074874878) (loss: 1.3209691047668457) 60% (loss: 1.3155345916748047) (loss: 3.553065776824951) (loss: 0.7651028633117676) (loss: 1.2417901754379272) (loss: 0.5090122222900391) 70% (loss: 1.9688977003097534) (loss: 1.2101274728775024) (loss: 0.7353891134262085) (loss: 4.496632099151611) (loss: 1.1977438926696777) 80% (loss: 0.6122779846191406) (loss: 0.1978452205657959) 90% (loss: 2.925973892211914) (loss: 0.7224714756011963) (loss: 3.4426732063293457) (loss: 0.864669680595398) (loss: 0.7429916858673096) 100% (loss: 0.9707019329071045) (loss: 1.3011910915374756) (loss: 1.6789230108261108) (loss: 1.5890564918518066) (loss: 1.098083734512329) 10% (loss: 0.8211327195167542) (loss: 0.3996143341064453) (loss: 1.0065710544586182) (loss: 1.1970899105072021) (loss: 0.5240126848220825) (loss: 2.003448247909546) 20% (loss: 0.41438865661621094) (loss: 1.2549121379852295) (loss: 6.7413177490234375) (loss: 1.0634278059005737) (loss: 0.3322247266769409) 30% (loss: 1.4010534286499023) (loss: 1.417870283126831) (loss: 2.320199489593506) (loss: 1.7494823932647705) (loss: 2.3497114181518555) (loss: 1.9314546585083008) 40% (loss: 0.7663705945014954) (loss: 0.6219184994697571) (loss: 1.5959246158599854) (loss: 0.8707460165023804) (loss: 0.8425863981246948) 50% (loss: 0.29785096645355225) (loss: 2.102315902709961) (loss: 0.7405308485031128) (loss: 2.986294984817505) (loss: 3.815967559814453) 60% (loss: 1.4300546646118164) (loss: 1.8566393852233887) (loss: 2.247467041015625) (loss: 2.7104740142822266) (loss: 1.4820632934570312) 70% (loss: 0.6761183142662048) (loss: 1.329371452331543) (loss: 0.2893921732902527) (loss: 0.6614304780960083) (loss: 1.0826709270477295) 80% (loss: 0.26484501361846924) (loss: 0.3368278741836548) 90% (loss: 0.30749717354774475) (loss: 0.9447928667068481) (loss: 1.7368628978729248) (loss: 0.5912729501724243) (loss: 0.21294079720973969) 100% (loss: 1.6755828857421875) (loss: 0.22065843641757965) (loss: 1.1223552227020264) (loss: 3.9443228244781494) (loss: 0.6578718423843384) 10% (loss: 1.1037558317184448) (loss: 1.1266727447509766) (loss: 2.8859479427337646) (loss: 2.0572962760925293) (loss: 0.8236886858940125) (loss: 0.8423221111297607) 20% (loss: 0.39242684841156006) (loss: 5.150489807128906) (loss: 0.5201625823974609) (loss: 0.5134903192520142) (loss: 2.9495270252227783) 30% (loss: 1.1873714923858643) (loss: 2.9178385734558105) (loss: 0.2883370518684387) (loss: 1.0432214736938477) (loss: 0.5771220922470093) (loss: 0.9665519595146179) 40% (loss: 1.688572883605957) (loss: 0.797522783279419) (loss: 0.4878343343734741) (loss: 0.9061266183853149) (loss: 0.7461610436439514) 50% (loss: 0.5096806883811951) (loss: 3.8605802059173584) (loss: 1.35743248462677) (loss: 0.9435357451438904) (loss: 2.646064519882202) 60% (loss: 1.0166975259780884) (loss: 3.453397750854492) (loss: 0.43569278717041016) (loss: 0.3899126648902893) (loss: 0.5948206782341003) 70% (loss: 1.2618355751037598) (loss: 0.875190794467926) (loss: 1.2282657623291016) (loss: 3.456099510192871) (loss: 0.5553568601608276) 80% (loss: 0.3235316276550293) (loss: 0.35681742429733276) 90% (loss: 0.46595293283462524) (loss: 3.335690498352051) (loss: 1.2321958541870117) (loss: 2.5816597938537598) (loss: 0.8420451879501343) 100% (loss: 0.6017984747886658) (loss: 1.2727534770965576) (loss: 0.7612038850784302) (loss: 3.7754173278808594) (loss: 2.0722711086273193) 10% (loss: 1.4646003246307373) (loss: 0.3221502900123596) (loss: 1.4025013446807861) (loss: 2.2677345275878906) (loss: 0.6136966943740845) (loss: 0.5455495715141296) 20% (loss: 1.137732982635498) (loss: 1.8602560758590698) (loss: 0.9113187789916992) (loss: 0.807198166847229) (loss: 2.3688461780548096) 30% (loss: 2.1433920860290527) (loss: 1.963822603225708) (loss: 0.7877256870269775) (loss: 1.2413313388824463) (loss: 4.074231147766113) (loss: 0.817785382270813) 40% (loss: 1.1431089639663696) (loss: 1.6361780166625977) (loss: 3.9385745525360107) (loss: 0.7163626551628113) (loss: 0.6228601932525635) 50% (loss: 0.33593523502349854) (loss: 0.3101949095726013) (loss: 4.418669700622559) (loss: 0.2939392626285553) (loss: 1.6995246410369873) 60% (loss: 3.139596939086914) (loss: 1.918969988822937) (loss: 2.1834211349487305) (loss: 0.550836443901062) (loss: 0.27146637439727783) 70% (loss: 0.7874791622161865) (loss: 2.873997211456299) (loss: 1.7545149326324463) (loss: 1.3819050788879395) (loss: 1.0478769540786743) 80% (loss: 2.1451168060302734) (loss: 0.5398515462875366) 90% (loss: 0.1836264282464981) (loss: 0.48886072635650635) (loss: 0.7255001664161682) (loss: 0.5122079849243164) (loss: 0.6639967560768127) 100% Final loss: 6.32108\n",
      "Training end: 2017-11-29 04:46:55.618205\n"
     ]
    }
   ],
   "source": [
    "# TODO REMOVE\n",
    "import datetime\n",
    "\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    print('Training start:', datetime.datetime.now())\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    for batch_inputs, batch_labels, is_last in skipgram_batches():\n",
    "        if is_last:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "            print('Training end:', datetime.datetime.now())\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            if (i % 250000 == 0):\n",
    "                print('(loss: {})'.format(loss_val), end=' ')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(word):\n",
    "    dists = np.abs(trained_embeddings - trained_embeddings[word_map[word], ]).sum(axis=1)\n",
    "    dists[word_map[word]] = 1e6\n",
    "    return unique_words[dists.argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_cos_neighbor(word):\n",
    "    dists = (np.dot(trained_embeddings, trained_embeddings[word_map[word],])\n",
    "             / np.linalg.norm(trained_embeddings) * np.linalg.norm(trained_embeddings[word_map[word],]))\n",
    "    dists[word_map[word]] = 1e6\n",
    "    return unique_words[dists.argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest word vectors for:\n",
      "cat: Topstock\n",
      "doctor: five-thirty\n",
      "cold: firemen\n",
      "blue: Raisonnable\n",
      "red: Fairies\n",
      "walk: books\n",
      "bring: case-study\n",
      "is: this\n",
      "Europe: Ugborough\n"
     ]
    }
   ],
   "source": [
    "print('Nearest word vectors for:')\n",
    "print('cat:', nearest_neighbor('cat'))\n",
    "print('doctor:', nearest_neighbor('doctor'))\n",
    "print('cold:', nearest_neighbor('cold'))\n",
    "print('blue:', nearest_neighbor('blue'))\n",
    "print('red:', nearest_neighbor('red'))\n",
    "print('walk:', nearest_neighbor('walk'))\n",
    "print('bring:', nearest_neighbor('bring'))\n",
    "print('is:', nearest_neighbor('is'))\n",
    "print('Europe:', nearest_neighbor('europe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest word vectors for:\n",
      "cat: facetiae\n",
      "doctor: SPATULAS/PALETTE\n",
      "cold: Pre-Windscale\n",
      "blue: Villers-aux-Noeuds\n",
      "red: LÃ¤cherlich\n",
      "walk: LIND\n",
      "bring: Bestowed\n",
      "is: FAMILY-ORIENTED\n",
      "Europe: Somerville-1941\n"
     ]
    }
   ],
   "source": [
    "print('Nearest word vectors for:')\n",
    "print('cat:', nearest_cos_neighbor('cat'))\n",
    "print('doctor:', nearest_cos_neighbor('doctor'))\n",
    "print('cold:', nearest_cos_neighbor('cold'))\n",
    "print('blue:', nearest_cos_neighbor('blue'))\n",
    "print('red:', nearest_cos_neighbor('red'))\n",
    "print('walk:', nearest_cos_neighbor('walk'))\n",
    "print('bring:', nearest_cos_neighbor('bring'))\n",
    "print('is:', nearest_cos_neighbor('is'))\n",
    "print('Europe:', nearest_cos_neighbor('europe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_W = Variable(torch.randn(embedding_size*2, embedding_size), requires_grad=True)\n",
    "enc_b = Variable(torch.zeros(1, embedding_size), requires_grad=True)\n",
    "dec_W = Variable(torch.randn(embedding_size, embedding_size*2), requires_grad=True)\n",
    "dec_b = Variable(torch.zeros(1, embedding_size*2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_node(child1, child2):\n",
    "    #\"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size). Return the encoding\n",
    "    #(1, embedding_size).\"\"\"\n",
    "    conc_embeds = Variable(torch.cat((child1.data, child2.data), 0))\n",
    "    # we use.view() because we need to make sure that the return value is a vector (as word embeddings),\n",
    "    # not a matrix\n",
    "    return conc_embeds.matmul(enc_W).add(enc_b).tanh().view(embedding_size)\n",
    "\n",
    "def decode_node(node):\n",
    "    # node is (1, embedding_size), output is (1, 2*embedding_size)\n",
    "    return node.matmul(dec_W).add(dec_b).tanh().view(embedding_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from random import choice, randint\n",
    "encoding_train_batch_size = 50 # number of sentences\n",
    "\n",
    "# Handle special treatment of parens by our parser.\n",
    "def nd_lbl(node):\n",
    "    if node.label() == '-LRB-':\n",
    "        return '('\n",
    "    elif node.label() == '-RRB-':\n",
    "        return ')'\n",
    "    else:\n",
    "        return node.label()\n",
    "\n",
    "# Note that node_encodings are passed by value, so we always modify the dictionary given to\n",
    "# the topmost function call.\n",
    "def encode_tree(node, node_encodings):\n",
    "    \"Encode_tree returns a pair of lists of partial derivatives for encoding matrix and bias\"\n",
    "    subtrees = [subtr for subtr in node]\n",
    "    if len(subtrees) == 0: # a leaf\n",
    "        if nd_lbl(node) in word_map:\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[word_map[nd_lbl(node)], ]))\n",
    "        else: # replace unknowns with a random word\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[randint(0, trained_embeddings.shape[0]), ]))\n",
    "    elif len(subtrees) == 1:\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = node_encodings[nd_lbl(subtrees[0])]\n",
    "    else:\n",
    "        if len(subtrees) != 2: # dbg\n",
    "            print(subtrees)\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        encode_tree(subtrees[1], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = encode_node(\n",
    "            node_encodings[nd_lbl(subtrees[0])],\n",
    "            node_encodings[nd_lbl(subtrees[1])])\n",
    "\n",
    "def make_parser():\n",
    "    parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "    parser.expect('Model loaded, ready to parse.')\n",
    "    return parser\n",
    "\n",
    "def kill_parser(parser):\n",
    "    parser.terminate()\n",
    "    \n",
    "def sentence_tree(sentence_form, parser):\n",
    "    parser.send(sentence_form+'\\n')\n",
    "    # (this secures us from finding one of the patterns below in the sentence itself:)\n",
    "    response = parser.expect([pexpect.TIMEOUT, 'ID'])\n",
    "    if response == 1: # can't happen if timed out\n",
    "        response = parser.expect(['Skipping sentence of length', '\\n\\(.*\\n', pexpect.TIMEOUT])\n",
    "    if response in [0, 2]:\n",
    "        return False\n",
    "    parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "    return ParentedTree.fromstring(clean_parser_output(parser_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 error:  14.511988639831543\n",
      "Batch 2 error:  15.387177467346191\n",
      "Batch 3 error:  15.15761947631836\n",
      "Batch 4 error:  10.777464866638184\n",
      "Batch 5 error:  10.480030059814453\n",
      "Batch 6 error:  12.86169719696045\n",
      "Batch 7 error:  5.273446083068848\n",
      "Batch 8 error:  6.528011322021484\n",
      "Batch 9 error:  4.766279220581055\n",
      "Batch 10 error:  3.2289998531341553\n",
      "Batch 11 error:  3.494234085083008\n",
      "Batch 12 error:  2.4390478134155273\n",
      "Batch 13 error:  2.455670118331909\n",
      "Batch 14 error:  3.6540942192077637\n",
      "Batch 15 error:  1.608689785003662\n",
      "Batch 16 error:  1.2823563814163208\n",
      "Batch 17 error:  0.6695910692214966\n",
      "Batch 18 error:  0.6050992012023926\n",
      "Batch 19 error:  1.7388299703598022\n",
      "Batch 20 error:  2.086718797683716\n",
      "Batch 21 error:  1.6035962104797363\n",
      "Batch 22 error:  0.6890740394592285\n",
      "Batch 23 error:  1.0876528024673462\n",
      "Batch 24 error:  0.924579918384552\n",
      "Batch 25 error:  0.6923951506614685\n"
     ]
    }
   ],
   "source": [
    "iters_n = 25\n",
    "encoding_train_batch_size = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iter_i in range(iters_n):\n",
    "    used_sents = [] # at least don't repeat them in one batch\n",
    "    batch_accum_error = 0\n",
    "    parser = make_parser()\n",
    "    nodes_n = 0 # count them to average the error\n",
    "    \n",
    "    for i in range(encoding_train_batch_size):\n",
    "        tree = False\n",
    "        # It's possible that sentence_tree() returns False, if the sentence was too long and\n",
    "        # rejected by the parser, or it timeouts.\n",
    "        while not tree:\n",
    "            sentence_n = randint(0, len(corpus_sents))\n",
    "            while sentence_n in used_sents:\n",
    "                sentence_n = randint(0, len(corpus_sents))\n",
    "            sentence = corpus_sents[sentence_n]\n",
    "            used_sents.append(sentence_n)\n",
    "            \n",
    "            sentence_form = ' '.join([unique_words[word_id] for word_id in sentence])\n",
    "            #print(sentence_n, sentence_form)\n",
    "            tree = sentence_tree(sentence_form, parser)\n",
    "\n",
    "        # Encode the tree.\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree, node_encodings)\n",
    "        \n",
    "        # Decode the tree back again.\n",
    "        # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "        # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "        node_decodings = dict()\n",
    "        node_decodings[nd_lbl(tree.root())] = node_encodings[nd_lbl(tree.root())]\n",
    "        nodes_to_visit = [ tree.root() ]\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            children = [child for child in current_node]\n",
    "            if len(children) == 0:\n",
    "                continue\n",
    "            elif len(children) == 2: # not a leaf\n",
    "                decoded_node = decode_node(node_decodings[nd_lbl(current_node)])\n",
    "                node_decodings[nd_lbl(children[0])] = decoded_node[:embedding_size]\n",
    "                node_decodings[nd_lbl(children[1])] = decoded_node[embedding_size:]\n",
    "                \n",
    "                #print(node_encodings[nd_lbl(current_node)])\n",
    "                #print(node_decodings[nd_lbl(current_node)])\n",
    "                err = node_encodings[nd_lbl(current_node)].sub(node_decodings[nd_lbl(current_node)]).abs().sum()\n",
    "                err.backward() # accumulate gradient\n",
    "                batch_accum_error += err.data\n",
    "                nodes_n += 1\n",
    "            else:\n",
    "                raise RuntimeError('unexpected number of node children in decode:' + str(children))\n",
    "        \n",
    "    kill_parser(parser)\n",
    "    print('Batch', iter_i+1, 'error: ', (batch_accum_error / nodes_n)[0])\n",
    "    if batch_accum_error[0] == 0:\n",
    "        for sentence_n in used_sents:\n",
    "            print(' '.join([unique_words[word_id] for word_id in corpus_sents[sentence_n]]))\n",
    "        raise RuntimeError\n",
    "    enc_W.data -= enc_W.grad.data * learning_rate\n",
    "    enc_b.data -= enc_b.grad.data * learning_rate\n",
    "    dec_W.data -= dec_W.grad.data * learning_rate\n",
    "    dec_b.data -= dec_b.grad.data * learning_rate\n",
    "    #print('ENC_W', enc_W.grad, 'ENC_B', enc_b.grad, 'DEC_W', dec_W.grad, 'DEC_B', dec_b.grad)\n",
    "    \n",
    "    enc_W.grad.data.zero_()\n",
    "    enc_b.grad.data.zero_()\n",
    "    dec_W.grad.data.zero_()\n",
    "    dec_b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/szymon/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentence_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', \"century's\", 'new', '\"', 'conan', '\"', 'and', 'that', \"he's\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co-writer/director', 'peter', \"jackson's\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', \"tolkien's\", 'middle-earth', '.'], ['effective', 'but', 'too-tepid', 'biopic']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "print(sentence_polarity.sents(categories='pos')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the sentence polarity corpus into test and training slices in proportion 10/90, just as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "assert len(sentence_polarity.sents(categories='pos')) == len(sentence_polarity.sents(categories='neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing start: 2017-11-29 12:44:05.242768\n"
     ]
    },
    {
     "ename": "EOF",
     "evalue": "End Of File (EOF). Exception style platform.\n<pexpect.pty_spawn.spawn object at 0x7f0be0060b00>\ncommand: /usr/bin/java\nargs: ['/usr/bin/java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model']\nbuffer (last 100 chars): b''\nbefore (last 100 chars): b\" blockbuster before midnight , you're going to face frightening late fees . o . k . , not really .\\r\\n\"\nafter: <class 'pexpect.exceptions.EOF'>\nmatch: None\nmatch_index: None\nexitstatus: None\nflag_eof: True\npid: 16175\nchild_fd: 77\nclosed: False\ntimeout: 30\ndelimiter: <class 'pexpect.exceptions.EOF'>\nlogfile: None\nlogfile_read: None\nlogfile_send: None\nmaxread: 2000\nignorecase: False\nsearchwindowsize: None\ndelaybeforesend: 0.05\ndelayafterclose: 0.1\ndelayafterterminate: 0.1\nsearcher: searcher_re:\n    0: TIMEOUT\n    1: re.compile(\"b'ID'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEOF\u001b[0m                                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEOF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'End Of File (EOF). Exception style platform.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOF\u001b[0m: End Of File (EOF). Exception style platform.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEOF\u001b[0m                                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7fcdb3f9b644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sentence too long, or times out the parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-3f063edec44a>\u001b[0m in \u001b[0;36msentence_tree\u001b[0;34m(sentence_form, parser)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_form\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# (this secures us from finding one of the patterns below in the sentence itself:)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIMEOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# can't happen if timed out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Skipping sentence of length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\(.*\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIMEOUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         return self.expect_list(compiled_pattern_list,\n\u001b[0;32m--> 321\u001b[0;31m                 timeout, searchwindowsize, async)\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     def expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTIMEOUT\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36meof\u001b[0;34m(self, err)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOF\u001b[0m: End Of File (EOF). Exception style platform.\n<pexpect.pty_spawn.spawn object at 0x7f0be0060b00>\ncommand: /usr/bin/java\nargs: ['/usr/bin/java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model']\nbuffer (last 100 chars): b''\nbefore (last 100 chars): b\" blockbuster before midnight , you're going to face frightening late fees . o . k . , not really .\\r\\n\"\nafter: <class 'pexpect.exceptions.EOF'>\nmatch: None\nmatch_index: None\nexitstatus: None\nflag_eof: True\npid: 16175\nchild_fd: 77\nclosed: False\ntimeout: 30\ndelimiter: <class 'pexpect.exceptions.EOF'>\nlogfile: None\nlogfile_read: None\nlogfile_send: None\nmaxread: 2000\nignorecase: False\nsearchwindowsize: None\ndelaybeforesend: 0.05\ndelayafterclose: 0.1\ndelayafterterminate: 0.1\nsearcher: searcher_re:\n    0: TIMEOUT\n    1: re.compile(\"b'ID'\")"
     ]
    }
   ],
   "source": [
    "train_pnt, test_pnt = 0, 0\n",
    "test_corp_len = (len(sentence_polarity.sents(categories='pos')) // 10\n",
    "                 + len(sentence_polarity.sents(categories='neg')) // 10)\n",
    "sent_pol_len = len(sentence_polarity.sents())\n",
    "\n",
    "train_sent_vecs = np.zeros((sent_pol_len - test_corp_len, embedding_size))\n",
    "test_sent_vecs = np.zeros((test_corp_len, embedding_size))\n",
    "train_sent_labels = np.zeros((sent_pol_len - test_corp_len, 1))\n",
    "test_sent_labels = np.zeros((test_corp_len, 1))\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "print('Parsing start:', datetime.datetime.now())\n",
    "for (label, sents) in [(1.0, sentence_polarity.sents(categories='pos')),\n",
    "                      (0.0, sentence_polarity.sents(categories='neg'))]:\n",
    "    sents = list(sents)\n",
    "    test_ids = sample(range(len(sents)), len(sents) // 10)\n",
    "    for sent_i in range(len(sents)):\n",
    "        tree = sentence_tree(' '.join(sents[sent_i]), parser)\n",
    "        if not tree: # sentence too long, or times out the parser\n",
    "            continue\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree, node_encodings)\n",
    "        if sent_i in test_ids:\n",
    "            #print(node_encodings[nd_lbl(tree.root())])\n",
    "            test_sent_vecs[test_pnt, :] = node_encodings[nd_lbl(tree.root())].data.numpy()\n",
    "            test_sent_labels[test_pnt, 0] = label\n",
    "            test_pnt += 1\n",
    "        else:\n",
    "            #print(node_encodings[nd_lbl(tree.root())])\n",
    "            train_sent_vecs[train_pnt, :] = node_encodings[nd_lbl(tree.root())].data.numpy()\n",
    "            train_sent_labels[train_pnt, 0] = label\n",
    "            train_pnt += 1\n",
    "kill_parser(parser)\n",
    "print('Parsing end:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper seems to use just a binary classifier of sentence vectors, without any neural net hidden layers. It is the approach we will try first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "regr_weights = tf.Variable(tf.random_normal([embedding_size, 1]))\n",
    "regr_bias = tf.Variable(tf.random_normal([1, 1]))\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, embedding_size], name='samples')\n",
    "Y = tf.placeholder(\"float\", [None, 1], name='labels')\n",
    "\n",
    "prediction = tf.sigmoid(tf.matmul(X, regr_weights) + regr_bias)\n",
    "loss = tf.losses.sigmoid_cross_entropy(prediction, Y)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.5).minimize(loss)\n",
    "\n",
    "acc = tf.metrics.accuracy(tf.round(prediction), tf.round(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.910725\n",
      "1000 0.506159\n",
      "2000 0.506093\n",
      "3000 0.506075\n",
      "4000 0.506053\n",
      "5000 0.505926\n",
      "6000 0.50588\n",
      "7000 0.505871\n",
      "8000 0.505868\n",
      "9000 0.505865\n",
      "10000 0.505864\n",
      "11000 0.505863\n",
      "12000 0.505862\n",
      "13000 0.505862\n",
      "14000 0.505861\n",
      "15000 0.505861\n",
      "16000 0.505861\n",
      "17000 0.505861\n",
      "18000 0.50586\n",
      "19000 0.50586\n",
      "20000 0.50586\n",
      "Test accuracy (0.0, 0.48592871)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20001\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer()) # needed for accuracy metric\n",
    "    for epoch_i in range(epochs):\n",
    "        _, curr_loss = sess.run([optimizer, loss],\n",
    "                                feed_dict={X: train_sent_vecs,\n",
    "                                           Y: train_sent_labels})\n",
    "        if epoch_i % 1000 == 0:\n",
    "            print(epoch_i, curr_loss)\n",
    "    \n",
    "    _, test_acc = sess.run([loss, acc], feed_dict={X: test_sent_vecs,\n",
    "                                          Y: test_sent_labels})\n",
    "    print('Test accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Try to do an autoencoder without loops: https://groups.google.com/forum/#!topic/theano-users/O5CM49-jMqQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
