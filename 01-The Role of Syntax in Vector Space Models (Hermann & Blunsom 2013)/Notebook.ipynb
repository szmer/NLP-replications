{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download and unpack `sentence polarity dataset v1.0` from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "2. Download BNC (TODO)\n",
    "3. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Ndadaye', 'Jarl', 'Raidan', 'speechlessly', '7°', 'shipowner', 'INTENSITY', 'Visio', '112½°']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\NP 0 2> (<L (S[dcl]\\NP)/NP POS POS chases (S[dcl]\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\NP 0 2> (<L (NP\\NP)/NP POS POS of (NP\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will run the underlying parser with pexpect, and intercept its outputs from within Python\n",
    "import pexpect\n",
    "parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "parser.expect('Model loaded, ready to parse.')\n",
    "parser.send('The cat chases a ball of yarn.\\n')\n",
    "parser.expect('ID')\n",
    "parser.expect('\\n\\(.*\\n')\n",
    "parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "print(parser_output)\n",
    "parser.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T_S[dcl]_1_2>\n",
      "  (<T_NP[nb]_0_2> (The ) (cat ))\n",
      "  (<T_S[dcl]\\NP_0_2>\n",
      "    (chases )\n",
      "    (<T_NP[nb]_0_2>\n",
      "      (<T_NP[nb]_0_2> (a ) (ball ))\n",
      "      (<T_NP\\NP_0_2> (of ) (<T_NP_0_1> (yarn. ))))))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "only_word = re.compile(r'<L\\s\\S+\\sPOS\\sPOS\\s(\\S+)\\s\\S+>')\n",
    "concat_label = re.compile(r'<(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)>')\n",
    "\n",
    "# some string cleanup\n",
    "def clean_parser_output(parse_output):\n",
    "    return concat_label.sub(lambda match: '<'+match.group(1)+'_'+match.group(2).replace('(', '[').replace(')', ']')\n",
    "                            +'_'+match.group(3)+'_'+match.group(4)+'>',\n",
    "                            only_word.sub(lambda match: match.group(1), parse_output))\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parser_output(parser_output))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `(parenthesized expression)`, the first item `(head)` is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = {word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sentences from the corpus, with words as their indices in the word map.\n",
    "corpus_sents = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            corpus_sents.append([])\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_sents[-1].append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([319948, 328721, 246274, 522645, 175160, 545942, 629992,  64627,\n",
      "       182598, 672074,  89054,  18919], dtype=int32), array([[160660],\n",
      "       [160660],\n",
      "       [160660],\n",
      "       [160660],\n",
      "       [545265],\n",
      "       [545265],\n",
      "       [545265],\n",
      "       [545265],\n",
      "       [272765],\n",
      "       [272765],\n",
      "       [272765],\n",
      "       [272765]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "def skipgrams_batch(batch_size):\n",
    "    assert batch_size % num_samples == 0\n",
    "    windows_n = batch_size // num_samples\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(windows_n):\n",
    "        target_sent = randint(0, len(corpus_sents)-1)\n",
    "        while len(corpus_sents[target_sent]) < 5:\n",
    "            target_sent = randint(0, len(corpus_sents)-1)\n",
    "        target = randint(2, len(corpus_sents[target_sent])-3)\n",
    "        for j in range(num_samples):\n",
    "            labels[i*num_samples+j][0] = corpus_sents[target_sent][target]\n",
    "        batch[i*num_samples] = corpus_sents[target_sent][target-2]\n",
    "        batch[i*num_samples+1] = corpus_sents[target_sent][target-1]\n",
    "        batch[i*num_samples+2] = corpus_sents[target_sent][target+1]\n",
    "        batch[i*num_samples+3] = corpus_sents[target_sent][target+2]\n",
    "        \n",
    "    return batch, labels\n",
    "\n",
    "print(skipgrams_batch(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(unique_words)\n",
    "embedding_size = 70\n",
    "\n",
    "# Model parameters.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The computation graph.\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "\n",
    "# Number of random words to sample apart from the true target; the model should learn to\n",
    "# assign low probability to them given the context.\n",
    "negative_samples_n = 64\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=labels,\n",
    "                                     inputs=embedding_layer,\n",
    "                                     num_sampled=negative_samples_n,\n",
    "                                     num_classes=vocabulary_size))\n",
    "optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.992\n",
      "63.6749\n",
      "10.3768\n",
      "3.23855\n",
      "7.22826\n",
      "8.20637\n",
      "7.24628\n",
      "7.81358\n",
      "2.36533\n",
      "5.33443\n",
      "3.36494\n",
      "4.38888\n",
      "8.69388\n",
      "6.57857\n",
      "1.57129\n",
      "2.89697\n",
      "3.04295\n",
      "2.89717\n",
      "3.25142\n",
      "2.1147\n",
      "4.2987\n",
      "3.13078\n",
      "Final loss: 3.7977\n",
      "(705241, 70)\n"
     ]
    }
   ],
   "source": [
    "steps_n = len(unique_words) * 3\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(steps_n):\n",
    "        batch_inputs, batch_labels = skipgrams_batch(batch_size)\n",
    "        if i+1 == steps_n:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            # TODO meaningful completion info\n",
    "            if (i % 100000 == 0):\n",
    "                print(loss_val)\n",
    "print(trained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.76175618, -0.48622197, -0.34541205, -0.99966902, -1.71851861,\n",
       "        1.11292863, -0.51043302,  1.39485371, -1.43934906, -2.27567148,\n",
       "        1.80909979, -1.66944766, -0.97985524, -1.38823986, -1.1190412 ,\n",
       "        2.17810202,  2.24670577, -0.43959919,  2.17650867, -0.39423934,\n",
       "        0.80785453,  1.88030303,  2.01537085, -1.87546027,  1.59910655,\n",
       "       -0.66886234, -1.32832623,  1.83181989,  0.70867652, -2.16132832,\n",
       "        1.50185454, -0.696419  , -0.92023361, -1.05840933, -0.50320876,\n",
       "        0.78315854,  0.67317289, -1.36801219,  1.37414455,  2.03041959,\n",
       "        1.46217203, -0.63557875,  1.68831849,  0.72084016,  1.09909523,\n",
       "       -0.79727054, -0.43951368,  0.78295988,  1.68929565,  1.90566957,\n",
       "        1.32585812,  2.26107812,  1.23102951, -1.14303839, -1.15728951,\n",
       "       -1.03613913, -1.71762002, -0.44431561,  1.55029309,  2.29587865,\n",
       "        1.72170174, -2.05274391, -1.26773238,  0.91030282,  0.40452766,\n",
       "        1.6505971 , -1.67897463,  0.41661876, -1.50299275, -2.19785619], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_embeddings[word_map['honey'], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b as l_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding_matrix = np.random.randn(embedding_size*2, embedding_size)\n",
    "#encoding_bias = np.zeros((1, embedding_size))\n",
    "#decoding_matrix = np.random.randn(embedding_size, embedding_size*2)\n",
    "#decoding_bias = np.zeros((1, embedding_size*2))\n",
    "init_enc_theta = np.random.randn(4, embedding_size, embedding_size*2)\n",
    "enc_theta_shape = (4, embedding_size, embedding_size*2)\n",
    "\n",
    "# Here the parameters need to passed as function arguments, because they will be optimized\n",
    "# by the LBFGS implementation.\n",
    "def encode_node(child1, child2, enc_theta):\n",
    "    #\"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size). Return the encoding\n",
    "    #(1, embedding_size) and partial derivatives for enc_mat and enc_bias.\"\"\"\n",
    "    conc_embeds = np.concatenate((child1, child2))\n",
    "    linear = np.dot(conc_embeds, np.transpose(enc_theta[0, :, :])) + enc_theta[1, :, 0] # enc_mat, enc_bias\n",
    "    d_tanh = 1 - np.tanh(linear) ** 2\n",
    "    d_mat = np.dot(np.reshape(conc_embeds, (conc_embeds.shape[0], 1)), np.reshape(d_tanh, (1, d_tanh.shape[0])))\n",
    "    return np.tanh(linear), d_mat, d_tanh\n",
    "\n",
    "def decode_node(node, enc_theta):\n",
    "    # node is (1, embedding_size), output is (1, 2*embedding_size)\n",
    "    linear = np.dot(node, enc_theta[2, :, :]) + enc_theta[3, 0, :] # dec_mat, dec_bias\n",
    "    d_tanh = 1 - np.tanh(linear) ** 2\n",
    "    d_mat = np.dot(np.reshape(node, (node.shape[0], 1)), np.reshape(d_tanh, (1, d_tanh.shape[0])))\n",
    "    return np.tanh(linear), d_mat, d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from random import choice, randint\n",
    "encoding_train_batch_size = 20 # number of sentences\n",
    "\n",
    "# Handle special treatment of parens by our parser.\n",
    "def nd_lbl(node):\n",
    "    if node.label() == '-LRB-':\n",
    "        return '('\n",
    "    elif node.label() == '-RRB-':\n",
    "        return ')'\n",
    "    else:\n",
    "        return node.label()\n",
    "\n",
    "# (just an implementation detail - we need to declare those in the global scope to\n",
    "# access them from node encoding function)\n",
    "batch_d_enc_mat = np.zeros((embedding_size*2, embedding_size))\n",
    "batch_d_enc_bias = np.zeros((1, embedding_size))\n",
    "batch_d_dec_mat = np.zeros((embedding_size, embedding_size*2))\n",
    "batch_d_dec_bias = np.zeros((1, embedding_size*2))\n",
    "\n",
    "# Note that node_encodings are passed by value, so we always modify the dictionary given to\n",
    "# the topmost function call.\n",
    "def encode_tree(node, enc_theta, node_encodings):\n",
    "    \"Encode_tree returns a pair of lists of partial derivatives for encoding matrix and bias\"\n",
    "    subtrees = [subtr for subtr in node]\n",
    "    if len(subtrees) == 0: # a leaf\n",
    "        if nd_lbl(node) in word_map:\n",
    "            node_encodings[nd_lbl(node)] = trained_embeddings[word_map[nd_lbl(node)], ]\n",
    "        else: # replace unknowns with a random word\n",
    "            node_encodings[nd_lbl(node)] = trained_embeddings[randint(0, trained_embeddings.shape[0]), ]\n",
    "        return ([], [])\n",
    "    elif len(subtrees) == 1:\n",
    "        encode_tree(subtrees[0], enc_theta, node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = node_encodings[nd_lbl(subtrees[0])]\n",
    "        return ([], [])\n",
    "    else:\n",
    "        if len(subtrees) != 2: # dbg\n",
    "            print(subtrees)\n",
    "        derivs = encode_tree(subtrees[0], enc_theta, node_encodings)\n",
    "        derivs2 = encode_tree(subtrees[1], enc_theta, node_encodings)\n",
    "        node_encodings[nd_lbl(node)], d_enc_mat, d_enc_bias = encode_node(\n",
    "            node_encodings[nd_lbl(subtrees[0])],\n",
    "            node_encodings[nd_lbl(subtrees[1])],\n",
    "            enc_theta)\n",
    "        return ( derivs[0] + derivs2[0] + [ d_enc_mat ],\n",
    "                 derivs[1] + derivs2[1] + [ d_enc_bias ])\n",
    "\n",
    "def make_parser():\n",
    "    parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "    parser.expect('Model loaded, ready to parse.')\n",
    "    return parser\n",
    "\n",
    "def kill_parser(parser):\n",
    "    parser.terminate()\n",
    "    \n",
    "def sentence_tree(sentence_form, parser):\n",
    "    parser.send(sentence_form+'\\n')\n",
    "    # (this secures us from finding one of the patterns below in the sentence itself:)\n",
    "    response = parser.expect([pexpect.TIMEOUT, 'ID'])\n",
    "    if response == 1: # can't happen if timed out\n",
    "        response = parser.expect(['Skipping sentence of length', '\\n\\(.*\\n', pexpect.TIMEOUT])\n",
    "    if response in [0, 2]:\n",
    "        return False\n",
    "    parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "    return ParentedTree.fromstring(clean_parser_output(parser_output))\n",
    "    \n",
    "def encoding_train_batch(enc_theta):\n",
    "    used_sents = [] # at least don't repeat them in one batch\n",
    "    batch_error = np.zeros((1, embedding_size))\n",
    "    # Partial derivatives -- to be stacked into a gradient at the end.\n",
    "    global batch_d_enc_mat, batch_d_enc_bias, batch_d_dec_mat, batch_d_dec_bias\n",
    "    batch_d_enc_mat = np.zeros((embedding_size*2, embedding_size))\n",
    "    batch_d_enc_bias = np.zeros((1, embedding_size))\n",
    "    batch_d_dec_mat = np.zeros((embedding_size, embedding_size*2))\n",
    "    batch_d_dec_bias = np.zeros((1, embedding_size*2))\n",
    "    # this is used to scale down the derivatives, ie. averaging\n",
    "    nodes_n = 0\n",
    "    enc_theta = np.reshape(enc_theta, enc_theta_shape)\n",
    "    \n",
    "    parser = make_parser()\n",
    "    \n",
    "    for i in range(encoding_train_batch_size):\n",
    "        tree = False\n",
    "        # It's possible that sentence_tree() returns False, if the sentence was too long and\n",
    "        # rejected by the parser, or it timeouts.\n",
    "        while not tree:\n",
    "            sentence_n = randint(0, len(corpus_sents))\n",
    "            while sentence_n in used_sents:\n",
    "                sentence_n = randint(0, len(corpus_sents))\n",
    "            sentence = corpus_sents[sentence_n]\n",
    "            used_sents.append(sentence_n)\n",
    "            \n",
    "            sentence_form = ' '.join([unique_words[word_id] for word_id in sentence])\n",
    "            #print(sentence_n, sentence_form)\n",
    "            tree = sentence_tree(sentence_form, parser)\n",
    "\n",
    "        # Encode the tree.\n",
    "        node_encodings = dict()\n",
    "        #print(clean_parse_output(parse_output))\n",
    "        (derivs_enc_mat, derivs_enc_bias) = encode_tree(tree, enc_theta, node_encodings)\n",
    "        #global batch_d_enc_mat, batch_d_enc_bias\n",
    "        batch_d_enc_mat = reduce(lambda d_sum, d_part: d_sum + d_part,\n",
    "                                 derivs_enc_mat, batch_d_enc_mat)\n",
    "        batch_d_enc_bias = reduce(lambda d_sum, d_part: d_sum + d_part,\n",
    "                                  derivs_enc_bias, batch_d_enc_bias)\n",
    "        \n",
    "        # Decode the tree back again.\n",
    "        # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "        # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "        node_decodings = dict()\n",
    "        node_decodings[nd_lbl(tree.root())] = node_encodings[nd_lbl(tree.root())]\n",
    "        encoding_errors = dict()\n",
    "        nodes_to_visit = [ tree.root() ]\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            children = [child for child in current_node]\n",
    "            if len(children) == 0:\n",
    "                continue\n",
    "            elif len(children) == 2: # not a leaf\n",
    "                decoded_node, d_dec_mat, d_dec_bias = decode_node(node_decodings[nd_lbl(current_node)],\n",
    "                                                                  enc_theta)\n",
    "                node_decodings[nd_lbl(children[0])] = decoded_node[:embedding_size]\n",
    "                node_decodings[nd_lbl(children[1])] = decoded_node[embedding_size:]\n",
    "                # Get the error and partial derivatives (both (1, 2*embedding_size)).\n",
    "                encoding_errors[nd_lbl(current_node)] = (node_encodings[nd_lbl(current_node)]\n",
    "                                                         - node_decodings[nd_lbl(current_node)])\n",
    "                batch_d_dec_mat = batch_d_dec_mat - d_dec_mat # dec is the minuend, so its part dev is -1\n",
    "                batch_d_dec_bias = batch_d_dec_bias - d_dec_bias\n",
    "                # np.abs()\n",
    "                #d_encoding = ((node_encodings[current_node] - node_decodings[current_node])\n",
    "                #              / encoding_errors[current_node])\n",
    "                #d_decoding = ((node_decodings[current_node] - node_encodings[current_node])\n",
    "                #              / encoding_errors[current_node])\n",
    "                #batch_d_enc_mat = batch_d_enc_mat + np.dot(d_encoding, sent_d_enc_mat[current_node])\n",
    "                #batch_d_enc_bias = batch_d_enc_bias + np.dot(d_encoding, sent_d_enc_bias[current_node])\n",
    "                nodes_n += 1\n",
    "            else:\n",
    "                print('unexpected number of node children in decode:', children)\n",
    "                return\n",
    "        \n",
    "        # Compute the error value.\n",
    "        sent_error = reduce(lambda err_sum, node_err: err_sum + node_err, encoding_errors.values(), np.zeros((1, embedding_size)))\n",
    "        sent_error = sent_error / len(encoding_errors)\n",
    "        # Update batch error.\n",
    "        batch_error = batch_error + sent_error / encoding_train_batch_size\n",
    "        \n",
    "    kill_parser(parser)\n",
    "    # TODO regularization    \n",
    "    batch_gradient = np.zeros((4, embedding_size, embedding_size*2))\n",
    "    batch_gradient[0, :, :] = np.transpose(batch_d_enc_mat) / nodes_n\n",
    "    batch_gradient[1, :, 0] = batch_d_enc_bias / nodes_n\n",
    "    batch_gradient[2, :, :] = batch_d_dec_mat / nodes_n\n",
    "    batch_gradient[3, 0, :] = batch_d_dec_bias / nodes_n\n",
    "    #[x / nodes_n for x in [batch_d_enc_mat, batch_d_enc_bias,\n",
    "    #                                        batch_d_dec_mat, batch_d_dec_bias]]\n",
    "    return batch_error, np.reshape(batch_gradient, (batch_gradient.size,)) # we need to return a 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.53868982,  0.00788834, -0.16616896, ..., -0.3556517 ,\n",
      "        0.18173019, -1.52697216]), array([[  2.15764155e-01,   4.50472088e-01,  -3.37242172e-01,\n",
      "          5.11381850e-01,  -3.34684135e-01,   3.32768988e-01,\n",
      "          9.71768843e-02,  -4.75634239e-01,   3.00089633e-02,\n",
      "          1.48959275e-07,   1.96650162e-01,   2.80371911e-01,\n",
      "          3.04179163e-01,   8.41766410e-02,   7.12384013e-02,\n",
      "          3.74978811e-01,   5.91840257e-01,   3.85963767e-01,\n",
      "          2.06987566e-01,  -7.55973052e-02,  -1.79390376e-01,\n",
      "          3.32294649e-02,   1.01937192e-01,   4.55952456e-01,\n",
      "         -1.01372252e-04,  -5.99674830e-01,   6.02569230e-01,\n",
      "          2.54261084e-01,   8.28629114e-04,  -9.27813045e-02,\n",
      "          2.79320505e-01,   2.02760039e-01,  -5.39431739e-01,\n",
      "          2.44421104e-01,   4.75181986e-02,   9.97902087e-02,\n",
      "         -1.99658934e-01,  -2.00739331e-01,   5.26042902e-01,\n",
      "         -3.03092527e-01,  -2.79803218e-01,  -1.47332131e-01,\n",
      "         -3.21314711e-01,  -3.71684252e-01,   5.98983335e-01,\n",
      "          1.26275911e-01,   4.64294216e-04,  -4.55174473e-01,\n",
      "          1.27028970e-01,   1.22750183e-03,  -1.88777251e-01,\n",
      "         -1.37789096e-01,   4.40044662e-02,  -6.06172335e-01,\n",
      "          6.64311143e-02,  -5.89071829e-01,   8.88491676e-02,\n",
      "         -1.89548964e-01,  -2.48386661e-01,  -3.41519644e-02,\n",
      "         -9.99925675e-02,  -4.20285266e-01,  -4.05921442e-01,\n",
      "         -1.85976607e-01,   4.86621030e-03,   3.81124472e-01,\n",
      "          9.22215614e-02,  -1.99889528e-01,   2.84019773e-01,\n",
      "          3.26054743e-01]]), {'warnflag': 0, 'nit': 3, 'funcalls': 16, 'grad': array([-0.44709665, -0.37676717, -1.41011834, ...,  0.        ,\n",
      "        0.        ,  0.        ]), 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'})\n"
     ]
    }
   ],
   "source": [
    "learned_enc_theta = l_bfgs(encoding_train_batch, init_enc_theta)\n",
    "print(learned_enc_theta)\n",
    "learned_enc_theta = np.reshape(learned_enc_theta[0], enc_theta_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/szymon/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentence_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', \"century's\", 'new', '\"', 'conan', '\"', 'and', 'that', \"he's\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co-writer/director', 'peter', \"jackson's\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', \"tolkien's\", 'middle-earth', '.'], ['effective', 'but', 'too-tepid', 'biopic']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "print(sentence_polarity.sents(categories='pos')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the sentence polarity corpus into test and training slices in proportion 10/90, just as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "assert len(sentence_polarity.sents(categories='pos')) == len(sentence_polarity.sents(categories='neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pnt, test_pnt = 0, 0\n",
    "test_corp_len = (len(sentence_polarity.sents(categories='pos')) // 10\n",
    "                 + len(sentence_polarity.sents(categories='neg')) // 10)\n",
    "sent_pol_len = len(sentence_polarity.sents())\n",
    "\n",
    "train_sent_vecs = np.zeros((sent_pol_len - test_corp_len, embedding_size))\n",
    "test_sent_vecs = np.zeros((test_corp_len, embedding_size))\n",
    "train_sent_labels = np.zeros((sent_pol_len - test_corp_len, 1))\n",
    "test_sent_labels = np.zeros((test_corp_len, 1))\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "for (label, sents) in [(1.0, sentence_polarity.sents(categories='pos')),\n",
    "                      (0.0, sentence_polarity.sents(categories='neg'))]:\n",
    "    sents = list(sents)\n",
    "    test_ids = sample(range(len(sents)), len(sents) // 10)\n",
    "    for sent_i in range(len(sents)):\n",
    "        tree = sentence_tree(' '.join(sents[sent_i]), parser)\n",
    "        if not tree: # sentence too long, or times out the parser\n",
    "            continue\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree, learned_enc_theta, node_encodings)\n",
    "        if sent_i in test_ids:\n",
    "            test_sent_vecs[test_pnt, :] = node_encodings[nd_lbl(tree.root())]\n",
    "            test_sent_labels[test_pnt, 0] = label\n",
    "            test_pnt += 1\n",
    "        else:\n",
    "            train_sent_vecs[train_pnt, :] = node_encodings[nd_lbl(tree.root())]\n",
    "            train_sent_labels[train_pnt, 0] = label\n",
    "            train_pnt += 1\n",
    "kill_parser(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper seems to use just a binary classifier of sentence vectors, without any neural net hidden layers. It is the approach we will try first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "regr_weights = tf.Variable(tf.random_normal([embedding_size, 1]))\n",
    "regr_bias = tf.Variable(tf.random_normal([1, 1]))\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, embedding_size], name='samples')\n",
    "Y = tf.placeholder(\"float\", [None, 1], name='labels')\n",
    "\n",
    "prediction = tf.sigmoid(tf.matmul(X, regr_weights) + regr_bias)\n",
    "loss = tf.losses.sigmoid_cross_entropy(prediction, Y)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "acc = tf.metrics.accuracy(tf.round(prediction), tf.round(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.627485\n",
      "1000 0.509037\n",
      "2000 0.507463\n",
      "3000 0.507022\n",
      "4000 0.506808\n",
      "5000 0.506669\n",
      "6000 0.506573\n",
      "7000 0.506515\n",
      "8000 0.50648\n",
      "9000 0.506456\n",
      "10000 0.506436\n",
      "11000 0.506416\n",
      "12000 0.506395\n",
      "13000 0.506375\n",
      "14000 0.506358\n",
      "15000 0.506344\n",
      "16000 0.506334\n",
      "17000 0.506326\n",
      "18000 0.506319\n",
      "19000 0.506311\n",
      "20000 0.506302\n",
      "Test accuracy (0.0, 0.49437147)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20001\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer()) # needed for accuracy metric\n",
    "    for epoch_i in range(epochs):\n",
    "        _, curr_loss = sess.run([optimizer, loss],\n",
    "                                feed_dict={X: train_sent_vecs,\n",
    "                                           Y: train_sent_labels})\n",
    "        if epoch_i % 1000 == 0:\n",
    "            print(epoch_i, curr_loss)\n",
    "    \n",
    "    _, test_acc = sess.run([loss, acc], feed_dict={X: test_sent_vecs,\n",
    "                                          Y: test_sent_labels})\n",
    "    print('Test accuracy', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
