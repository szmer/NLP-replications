{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download and unpack `sentence polarity dataset v1.0` from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "2. Download BNC (TODO)\n",
    "3. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'telegrams/telexes/faxes/express', '91â€“100%', 'toques', 'rush-strewn', 'Angerer', 'Big-Endians', 'Sayed', 'WARMER', 'Brews']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\NP 0 2> (<L (S[dcl]\\NP)/NP POS POS chases (S[dcl]\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\NP 0 2> (<L (NP\\NP)/NP POS POS of (NP\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will run the underlying parser with pexpect, and intercept its outputs from within Python\n",
    "import pexpect\n",
    "parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "parser.expect('Model loaded, ready to parse.')\n",
    "parser.send('The cat chases a ball of yarn.\\n')\n",
    "parser.expect('ID')\n",
    "parser.expect('\\n\\(.*\\n')\n",
    "parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "print(parser_output)\n",
    "parser.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T_S[dcl]_1_2>\n",
      "  (<T_NP[nb]_0_2> (The ) (cat ))\n",
      "  (<T_S[dcl]\\NP_0_2>\n",
      "    (chases )\n",
      "    (<T_NP[nb]_0_2>\n",
      "      (<T_NP[nb]_0_2> (a ) (ball ))\n",
      "      (<T_NP\\NP_0_2> (of ) (<T_NP_0_1> (yarn. ))))))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "only_word = re.compile(r'<L\\s\\S+\\sPOS\\sPOS\\s(\\S+)\\s\\S+>')\n",
    "concat_label = re.compile(r'<(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)>')\n",
    "\n",
    "# some string cleanup\n",
    "def clean_parser_output(parse_output):\n",
    "    return concat_label.sub(lambda match: '<'+match.group(1)+'_'+match.group(2).replace('(', '[').replace(')', ']')\n",
    "                            +'_'+match.group(3)+'_'+match.group(4)+'>',\n",
    "                            only_word.sub(lambda match: match.group(1), parse_output))\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parser_output(parser_output))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `(parenthesized expression)`, the first item `(head)` is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = { word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all sentences from the corpus, with words as their indices in the word map.\n",
    "corpus_sents = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            corpus_sents.append([])\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_sents[-1].append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "vocabulary_size = len(unique_words) + 1 # add the boundary token\n",
    "embedding_size = 128\n",
    "batch_size = 128\n",
    "# Number of sample correct word pairs to be shown to word2vec for one random target word.\n",
    "num_samples = 4\n",
    "assert num_samples % 2 == 0\n",
    "assert batch_size % num_samples == 0\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vocabulary_size - 1\n",
    "\n",
    "corp_runs = 6\n",
    "sent_step = 1 # we train 1/sent_step of all the sentences\n",
    "\n",
    "def skipgram_batches():\n",
    "    for run_n in range(corp_runs):\n",
    "        sent_n = 0\n",
    "        word_n = 0\n",
    "        \n",
    "        target_n = 0 # relative to the current batch\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        while sent_n < len(corpus_sents):\n",
    "            for j in range(num_samples):\n",
    "                batch[target_n*num_samples+j] = corpus_sents[sent_n][word_n]\n",
    "            # \"Good\" examples - words near the target (we will let TensorFlow randomize the \"bad\" ones)\n",
    "            for j in range(num_samples // 2):\n",
    "                labels[target_n*num_samples+j*2][0] = (corpus_sents[sent_n][word_n-j-1] if word_n-j-1 >= 0\n",
    "                                                       else bound_token_id)\n",
    "                labels[target_n*num_samples+j*2+1][0] = (corpus_sents[sent_n][word_n+j+1]\n",
    "                                                         if word_n+j+1 < len(corpus_sents[sent_n])\n",
    "                                                         else bound_token_id)\n",
    "                \n",
    "            target_n += 1\n",
    "            if target_n == (batch_size // num_samples):\n",
    "                yield batch, labels, False\n",
    "                batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "                target_n = 0\n",
    "                \n",
    "            word_n += 1\n",
    "            try:\n",
    "                while word_n == len(corpus_sents[sent_n]):\n",
    "                    word_n = 0\n",
    "                    sent_n += sent_step\n",
    "                    if (floor(sent_n / len(corpus_sents) * 10)\n",
    "                        > floor((sent_n-sent_step) / len(corpus_sents) * 10)):\n",
    "                        print('{}0%'.format(floor(sent_n / len(corpus_sents) * 10)), end=' ')\n",
    "            except IndexError: # happens on the end of the corpus\n",
    "                break\n",
    "                \n",
    "        batch[target_n:] = 0.0\n",
    "        labels[target_n:, :] = 0.0\n",
    "        yield batch, labels, (run_n == corp_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    # Model parameters: word embeddings and model weights & biases for each word.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # The computation graph.\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "    \n",
    "    # Number of random words to sample apart from the true target; the model should learn to\n",
    "    # assign low probability to them given the context.\n",
    "    negative_samples_n = batch_size\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=labels,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         num_sampled=negative_samples_n,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    # Vanilla SGD seems to work here better - since we train practically a different word vector\n",
    "    # each time, decaying momentum hinders training of later vectors before they can even be shown\n",
    "    # to the net, especially in the case of Adagrad's vanishing updates.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2017-12-10 02:20:30.649435\n",
      "(loss: 756.727783203125) (loss: 0.83027184009552) 10% (loss: 11.908158302307129) 20% (loss: 1.0684494972229004) (loss: 13.583332061767578) 30% (loss: 3.6548843383789062) 40% (loss: 0.25112614035606384) 50% (loss: 1.2839581966400146) 60% (loss: 102.95244598388672) (loss: 2.302624225616455) 70% (loss: 0.3007936179637909) 80% (loss: 0.3345237970352173) 90% (loss: 0.7518746852874756) 100% (loss: 1.0946375131607056) 10% (loss: 0.7554669380187988) 20% (loss: 3.34639310836792) (loss: 0.5009531378746033) 30% (loss: 1.2865395545959473) 40% (loss: 0.3014761209487915) (loss: 0.8342760801315308) 50% (loss: 0.8358651399612427) 60% (loss: 1.373337745666504) 70% (loss: 0.6899360418319702) 80% (loss: 0.18796537816524506) 90% (loss: 1.2796738147735596) 100% (loss: 2.521474599838257) 10% (loss: 0.6453143358230591) (loss: 0.6291516423225403) 20% (loss: 1.1040339469909668) 30% (loss: 1.1350901126861572) 40% (loss: 2.5494096279144287) (loss: 0.5065284967422485) 50% (loss: 0.5129037499427795) 60% (loss: 0.6891321539878845) 70% (loss: 1.0389413833618164) (loss: 0.964057207107544) 80% 90% (loss: 1.2843424081802368) 100% (loss: 1.441727876663208) (loss: 0.8627890348434448) 10% (loss: 0.7150441408157349) 20% (loss: 1.6866240501403809) 30% (loss: 0.8250105381011963) (loss: 1.1183265447616577) 40% (loss: 0.6904537677764893) 50% (loss: 1.6663753986358643) 60% (loss: 0.5772422552108765) (loss: 0.5508003830909729) 70% (loss: 0.7592222690582275) 80% 90% (loss: 0.20749668776988983) (loss: 0.36294394731521606) 100% (loss: 0.85919189453125) 10% (loss: 1.259019136428833) 20% (loss: 2.458725929260254) (loss: 0.6384924650192261) 30% (loss: 1.160318374633789) 40% (loss: 0.7564567923545837) 50% (loss: 1.1823275089263916) 60% (loss: 1.1618924140930176) (loss: 0.6062015891075134) 70% (loss: 0.7169199585914612) 80% (loss: 0.5445224046707153) 90% (loss: 0.7589182257652283) 100% (loss: 0.684209942817688) 10% (loss: 0.8367310762405396) (loss: 2.3474292755126953) 20% (loss: 1.06565260887146) 30% (loss: 0.5183948278427124) 40% (loss: 0.5190812945365906) (loss: 0.942033588886261) 50% (loss: 0.3821859657764435) 60% (loss: 0.46928805112838745) 70% (loss: 0.3597695827484131) 80% (loss: 0.5187564492225647) 90% (loss: 0.3101125955581665) 100% Final loss: 11.9557\n",
      "Training end: 2017-12-10 12:44:52.513319\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    print('Training start:', datetime.datetime.now())\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    for batch_inputs, batch_labels, is_last in skipgram_batches():\n",
    "        if is_last:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "            print('Training end:', datetime.datetime.now())\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            if (i % 250000 == 0):\n",
    "                print('(loss: {})'.format(loss_val), end=' ')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbor(word):\n",
    "    dists = np.abs(trained_embeddings - trained_embeddings[word_map[word], ]).sum(axis=1)\n",
    "    dists[word_map[word]] = 1e6\n",
    "    return unique_words[dists.argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest word vectors for:\n",
      "cat: dog\n",
      "doctor: nurse\n",
      "cold: hot\n",
      "blue: dark\n",
      "red: yellow\n",
      "walk: talk\n",
      "bring: brought\n",
      "is: was\n",
      "Europe: clinic\n"
     ]
    }
   ],
   "source": [
    "print('Nearest word vectors for:')\n",
    "print('cat:', nearest_neighbor('cat'))\n",
    "print('doctor:', nearest_neighbor('doctor'))\n",
    "print('cold:', nearest_neighbor('cold'))\n",
    "print('blue:', nearest_neighbor('blue'))\n",
    "print('red:', nearest_neighbor('red'))\n",
    "print('walk:', nearest_neighbor('walk'))\n",
    "print('bring:', nearest_neighbor('bring'))\n",
    "print('is:', nearest_neighbor('is'))\n",
    "print('Europe:', nearest_neighbor('europe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('trained_embeddings.csv', trained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_W = Variable(torch.randn(embedding_size*2, embedding_size), requires_grad=True)\n",
    "enc_b = Variable(torch.zeros(1, embedding_size), requires_grad=True)\n",
    "dec_W = Variable(torch.randn(embedding_size, embedding_size*2), requires_grad=True)\n",
    "dec_b = Variable(torch.zeros(1, embedding_size*2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_node(child1, child2):\n",
    "    #\"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size). Return the encoding\n",
    "    #(1, embedding_size).\"\"\"\n",
    "    conc_embeds = Variable(torch.cat((child1.data, child2.data), 0))\n",
    "    # we use.view() because we need to make sure that the return value is a vector (as word embeddings),\n",
    "    # not a matrix\n",
    "    return conc_embeds.matmul(enc_W).add(enc_b).tanh().view(embedding_size)\n",
    "\n",
    "def decode_node(node):\n",
    "    # node is (1, embedding_size), output is (1, 2*embedding_size)\n",
    "    return node.matmul(dec_W).add(dec_b).tanh().view(embedding_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from random import choice, randint\n",
    "encoding_train_batch_size = 50 # number of sentences\n",
    "\n",
    "# Handle special treatment of parens by our parser.\n",
    "def nd_lbl(node):\n",
    "    if node.label() == '-LRB-':\n",
    "        return '('\n",
    "    elif node.label() == '-RRB-':\n",
    "        return ')'\n",
    "    else:\n",
    "        return node.label()\n",
    "\n",
    "# Note that node_encodings are passed by value, so we always modify the dictionary given to\n",
    "# the topmost function call.\n",
    "def encode_tree(node, node_encodings):\n",
    "    \"Encode_tree returns a pair of lists of partial derivatives for encoding matrix and bias\"\n",
    "    subtrees = [subtr for subtr in node]\n",
    "    if len(subtrees) == 0: # a leaf\n",
    "        if nd_lbl(node) in word_map:\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[word_map[nd_lbl(node)], ]))\n",
    "        else: # replace unknowns with a random word\n",
    "            node_encodings[nd_lbl(node)] = Variable(\n",
    "                torch.from_numpy(trained_embeddings[randint(0, trained_embeddings.shape[0]), ]))\n",
    "    elif len(subtrees) == 1:\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = node_encodings[nd_lbl(subtrees[0])]\n",
    "    else:\n",
    "        if len(subtrees) != 2: # dbg\n",
    "            print(subtrees)\n",
    "        encode_tree(subtrees[0], node_encodings)\n",
    "        encode_tree(subtrees[1], node_encodings)\n",
    "        node_encodings[nd_lbl(node)] = encode_node(\n",
    "            node_encodings[nd_lbl(subtrees[0])],\n",
    "            node_encodings[nd_lbl(subtrees[1])])\n",
    "\n",
    "def make_parser():\n",
    "    parser = pexpect.spawn('java -jar easyccg-0.2/easyccg.jar --model easyccg-0.2/model')\n",
    "    parser.expect('Model loaded, ready to parse.')\n",
    "    return parser\n",
    "\n",
    "def kill_parser(parser):\n",
    "    parser.terminate()\n",
    "    \n",
    "def sentence_tree(sentence_form, parser):\n",
    "    parser.send(sentence_form+'\\n')\n",
    "    # (this secures us from finding one of the patterns below in the sentence itself:)\n",
    "    response = parser.expect([pexpect.TIMEOUT, 'ID', pexpect.EOF])\n",
    "    if response == 1: # can't happen if timed out\n",
    "        response = parser.expect(['Skipping sentence of length', '\\n\\(.*\\n', pexpect.TIMEOUT])\n",
    "    if response == 0: # timeout, pass\n",
    "        return False\n",
    "    if response == 2:\n",
    "        print('received EOF from the parser on', sentence_form)\n",
    "        raise RuntimeError('parser died')\n",
    "    parser_output = parser.after.decode().strip() # encode from bytes into str, strip whitespace\n",
    "    return ParentedTree.fromstring(clean_parser_output(parser_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser has to be a global, so tree generators can restart it in case of problems,\n",
    "parser = None\n",
    "\n",
    "def run_autoencoder(feed_generator, learning_rate=1.0,\n",
    "        # The following can be supplied for the additional task of sentiment binary classification.\n",
    "        sentim_classif_fun=None, sentim_W=None, sentim_b=None,\n",
    "        # for evaluating sentiment classification:\n",
    "        enable_training=True, measure_accuracy=False):\n",
    "    \n",
    "    def commit_gradients():\n",
    "        # Perform the gradient descent.\n",
    "        enc_W.data -= enc_W.grad.data * learning_rate\n",
    "        enc_b.data -= enc_b.grad.data * learning_rate\n",
    "        dec_W.data -= dec_W.grad.data * learning_rate\n",
    "        dec_b.data -= dec_b.grad.data * learning_rate\n",
    "            \n",
    "        # Clear the gradient after applying.\n",
    "        enc_W.grad.data.zero_()\n",
    "        enc_b.grad.data.zero_()\n",
    "        dec_W.grad.data.zero_()\n",
    "        dec_b.grad.data.zero_()\n",
    "    \n",
    "    global parser\n",
    "    parser = make_parser()\n",
    "    accuracy = 0\n",
    "\n",
    "    tree_n = 0\n",
    "    for tree, sentence in feed_generator:\n",
    "        tree_n += 1\n",
    "        tree_accum_error = 0.0\n",
    "        tree_node_n = 0 # count the nodes from all trees to average the error\n",
    "\n",
    "        # Encode the tree.\n",
    "        node_encodings = dict()\n",
    "        encode_tree(tree.root(), node_encodings)\n",
    "        \n",
    "        # Decode the tree back again.\n",
    "        # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "        # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "        node_decodings = dict()\n",
    "        node_decodings[nd_lbl(tree.root())] = node_encodings[nd_lbl(tree.root())]\n",
    "        nodes_to_visit = [ tree.root() ]\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            children = [child for child in current_node]\n",
    "            if len(children) in [0, 1]: # leaf\n",
    "                continue\n",
    "            elif len(children) == 2: # not a leaf\n",
    "                decoded_node = decode_node(node_decodings[nd_lbl(current_node)])\n",
    "                node_decodings[nd_lbl(children[0])] = decoded_node[:embedding_size]\n",
    "                node_decodings[nd_lbl(children[1])] = decoded_node[embedding_size:]\n",
    "                \n",
    "                nodes_to_visit += children\n",
    "                \n",
    "                if enable_training:\n",
    "                    err = node_encodings[nd_lbl(current_node)].sub(node_decodings[nd_lbl(current_node)]).abs().sum()\n",
    "                    err.backward(retain_graph=True) # accumulate gradient\n",
    "                    commit_gradients()\n",
    "                    tree_accum_error += err.data\n",
    "                tree_node_n += 1\n",
    "            else:\n",
    "                raise RuntimeError('unexpected number of node children ({}) in decode:'.format(\n",
    "                    len(children), str(current_node)))\n",
    "                \n",
    "        # Additional task - sentiment classification.\n",
    "        if sentim_classif_fun:\n",
    "            y = Variable(torch.Tensor([ sentim_classif_fun(sentence) ]))\n",
    "            y_pred = node_encodings[nd_lbl(tree.root())].matmul(sentim_W).add(sentim_b).sigmoid()\n",
    "            classif_err = y.add(- y_pred).abs()\n",
    "            \n",
    "            if measure_accuracy:\n",
    "                if y.data[0] == y_pred.data[0][0]:\n",
    "                    accuracy += 1\n",
    "            \n",
    "            if enable_training:\n",
    "                classif_err.backward(retain_graph=True)\n",
    "                \n",
    "                sentim_W.data -= sentim_W.grad.data * learning_rate\n",
    "                sentim_b.data -= sentim_b.grad.data * learning_rate\n",
    "                sentim_W.grad.data.zero_()\n",
    "                sentim_b.grad.data.zero_()\n",
    "                commit_gradients()\n",
    "            \n",
    "        if tree_node_n <= 1:\n",
    "            continue\n",
    "        \n",
    "        if enable_training and tree_n % 75 == 0:\n",
    "            print('Error metric: ', (tree_accum_error / tree_node_n)[0])\n",
    "\n",
    "    kill_parser(parser)\n",
    "    if measure_accuracy:\n",
    "        print('Accuracy: {}'.format(accuracy / tree_n))\n",
    "    parser = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnc_trees_batch_size = 101\n",
    "\n",
    "def bnc_trees_batch():\n",
    "    global parser\n",
    "    for i in range(bnc_trees_batch_size):\n",
    "        tree = False\n",
    "        # It's possible that sentence_tree() returns False, if the sentence was too long and\n",
    "        # rejected by the parser, or it timeouts.\n",
    "        sentence_form = ''\n",
    "        while not tree:\n",
    "            sentence_n = randint(0, len(corpus_sents))\n",
    "            while sentence_n in used_sents:\n",
    "                sentence_n = randint(0, len(corpus_sents))\n",
    "            sentence = corpus_sents[sentence_n]\n",
    "            used_sents.append(sentence_n)\n",
    "            \n",
    "            sentence_form = ' '.join([unique_words[word_id] for word_id in sentence])\n",
    "            #print(sentence_n, sentence_form)\n",
    "            try:\n",
    "                tree = sentence_tree(sentence_form, parser)\n",
    "            except RuntimeError as err: # parser died\n",
    "                if err.args[0] == 'parser died':\n",
    "                    parser = make_parser()\n",
    "                    tree = False\n",
    "                    continue\n",
    "                else:\n",
    "                    raise RuntimeError(msg)\n",
    "        yield tree, sentence_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error metric:  82.29735565185547\n",
      "Error metric:  119.57599639892578\n",
      "Error metric:  124.4228515625\n",
      "Error metric:  119.85279083251953\n",
      "Error metric:  119.9255142211914\n",
      "Error metric:  67.9533462524414\n",
      "Error metric:  105.62971496582031\n",
      "Error metric:  115.7391128540039\n",
      "Error metric:  124.64643859863281\n",
      "Error metric:  123.59395599365234\n",
      "Error metric:  121.17572784423828\n",
      "Error metric:  60.99999237060547\n",
      "Error metric:  127.54508209228516\n",
      "Error metric:  121.99143981933594\n",
      "Error metric:  124.6204605102539\n",
      "Error metric:  112.07550811767578\n",
      "Error metric:  129.04229736328125\n",
      "Error metric:  120.79609680175781\n",
      "Error metric:  102.4010238647461\n",
      "Error metric:  105.70718383789062\n",
      "Error metric:  106.33282470703125\n",
      "Error metric:  58.47898483276367\n",
      "Error metric:  119.68660736083984\n",
      "Error metric:  72.68099212646484\n",
      "Error metric:  111.3286361694336\n",
      "Error metric:  114.75283813476562\n",
      "Error metric:  114.69685363769531\n",
      "Error metric:  110.8473892211914\n",
      "Error metric:  120.12096405029297\n",
      "Error metric:  90.99771118164062\n",
      "Error metric:  120.6380844116211\n",
      "Error metric:  124.8847885131836\n",
      "Error metric:  116.79491424560547\n",
      "Error metric:  110.14727783203125\n",
      "Error metric:  132.29896545410156\n",
      "Error metric:  125.93389129638672\n",
      "Error metric:  99.13243865966797\n",
      "Error metric:  118.42890930175781\n",
      "Error metric:  135.57876586914062\n",
      "Error metric:  109.96411895751953\n",
      "Error metric:  121.5030517578125\n",
      "Error metric:  125.67503356933594\n",
      "Error metric:  123.19194793701172\n",
      "Error metric:  111.28870391845703\n",
      "Error metric:  126.94532775878906\n",
      "Error metric:  128.6094512939453\n",
      "Error metric:  121.92509460449219\n",
      "Error metric:  121.70418548583984\n",
      "Error metric:  127.53240203857422\n",
      "Error metric:  119.28474426269531\n",
      "Error metric:  127.6810073852539\n",
      "Error metric:  119.34197998046875\n",
      "Error metric:  88.99858856201172\n",
      "Error metric:  127.56671142578125\n",
      "Error metric:  124.68255615234375\n",
      "Error metric:  124.45589447021484\n",
      "Error metric:  125.58940887451172\n",
      "Error metric:  107.248779296875\n",
      "Error metric:  122.10163116455078\n",
      "Error metric:  110.7904052734375\n",
      "Error metric:  98.0\n",
      "Error metric:  125.50689697265625\n",
      "Error metric:  113.63639831542969\n",
      "Error metric:  124.34503936767578\n",
      "Error metric:  118.07488250732422\n",
      "Error metric:  122.9240493774414\n",
      "Error metric:  118.35295104980469\n",
      "Error metric:  115.98495483398438\n",
      "Error metric:  115.38754272460938\n",
      "Error metric:  121.05020141601562\n",
      "Error metric:  132.22222900390625\n",
      "Error metric:  132.9449920654297\n",
      "Error metric:  121.47987365722656\n",
      "Error metric:  118.2066421508789\n",
      "Error metric:  129.00808715820312\n",
      "Error metric:  121.52484130859375\n",
      "Error metric:  119.10839080810547\n",
      "Error metric:  94.43132781982422\n",
      "Error metric:  130.03501892089844\n",
      "Error metric:  60.0\n",
      "Error metric:  111.1993637084961\n",
      "Error metric:  122.74391174316406\n",
      "Error metric:  69.39392852783203\n",
      "Error metric:  113.10218811035156\n",
      "Error metric:  126.27425384521484\n",
      "Error metric:  68.03155517578125\n",
      "Error metric:  122.2085952758789\n",
      "Error metric:  56.83268356323242\n",
      "Error metric:  117.06764221191406\n",
      "Error metric:  83.33341217041016\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.2\n",
    "iters_n = 50 # number of batches to be fed to the autoencoder from BNC alone\n",
    "\n",
    "for i in range(iters_n):\n",
    "    feed_gen = bnc_trees_batch()\n",
    "    run_autoencoder(feed_gen, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', \"century's\", 'new', '\"', 'conan', '\"', 'and', 'that', \"he's\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "pos_sents = []\n",
    "neg_sents = []\n",
    "with io.open('rt-polaritydata/rt-polaritydata/rt-polarity.pos', encoding='latin-1') as pos_file:\n",
    "    pos_sents = [line.split() for line in pos_file.read().split('\\n')]\n",
    "with io.open('rt-polaritydata/rt-polaritydata/rt-polarity.neg', encoding='latin-1') as neg_file:\n",
    "    neg_sents = [line.split() for line in neg_file.read().split('\\n')]\n",
    "    \n",
    "print(pos_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the sentence polarity corpus into test and training slices in proportion 10/90, just as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "assert len(pos_sents) == len(neg_sents)\n",
    "test_ids = sample(range(len(sents)), len(sents) // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_polarity_feed(is_test=False):\n",
    "    global parser\n",
    "    for sent_i in range(len(pos_sents)):\n",
    "        if (sent_i in test_ids) != is_test:\n",
    "                continue\n",
    "        \n",
    "        # We have to alternate the positive and negative sentences at least for the training phase.\n",
    "        for sent_corp in [pos_sents, neg_sents]:\n",
    "            try:\n",
    "                tree = sentence_tree(' '.join(sent_corp[sent_i]), parser)\n",
    "            except RuntimeError as err:\n",
    "                if err.args[0] == 'parser died':\n",
    "                    parser = make_parser()\n",
    "                    tree = False\n",
    "                else:\n",
    "                    raise RuntimeError(err)\n",
    "                    \n",
    "            if tree:\n",
    "                #tree.pprint()\n",
    "                yield tree, ' '.join(sent_corp[sent_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polarity_dict = { }\n",
    "for sent in pos_sents:\n",
    "    polarity_dict[' '.join(sent)] = 1.0\n",
    "for sent in neg_sents:\n",
    "    polarity_dict[' '.join(sent)] = 0.0\n",
    "\n",
    "def get_sent_polarity(sent):\n",
    "    return polarity_dict[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error metric:  133.8715362548828\n",
      "Error metric:  132.85879516601562\n",
      "Error metric:  124.361328125\n",
      "Error metric:  119.7922592163086\n",
      "Error metric:  128.8044891357422\n",
      "Error metric:  108.0\n",
      "Error metric:  116.0915298461914\n",
      "Error metric:  108.21324920654297\n",
      "Error metric:  110.42928314208984\n",
      "Error metric:  98.72684478759766\n",
      "Error metric:  111.57719421386719\n",
      "Error metric:  92.7457046508789\n",
      "Error metric:  118.29249572753906\n",
      "Error metric:  122.05277252197266\n",
      "Error metric:  127.01619720458984\n",
      "Error metric:  119.55056762695312\n",
      "Error metric:  118.27478790283203\n",
      "Error metric:  128.71327209472656\n",
      "Error metric:  123.3331298828125\n",
      "Error metric:  124.17900848388672\n",
      "Error metric:  75.33333587646484\n",
      "Error metric:  113.25749206542969\n",
      "Error metric:  116.8322982788086\n",
      "Error metric:  128.8441162109375\n",
      "Error metric:  59.000083923339844\n",
      "Error metric:  127.61494445800781\n",
      "Error metric:  124.37112426757812\n",
      "Error metric:  118.12250518798828\n",
      "Error metric:  114.53072357177734\n",
      "Error metric:  124.99211883544922\n",
      "Error metric:  118.61942291259766\n",
      "Error metric:  122.98011779785156\n",
      "Error metric:  86.0\n",
      "Error metric:  114.7484359741211\n",
      "Error metric:  118.84625244140625\n",
      "Error metric:  113.9508285522461\n",
      "Error metric:  115.53827667236328\n",
      "Error metric:  120.6219711303711\n",
      "Error metric:  124.65788269042969\n",
      "Error metric:  109.41130828857422\n",
      "Error metric:  117.35317993164062\n",
      "Error metric:  73.15608978271484\n",
      "Error metric:  117.96234130859375\n",
      "Error metric:  122.28571319580078\n",
      "Error metric:  129.12265014648438\n",
      "Error metric:  121.24699401855469\n",
      "Error metric:  125.91986846923828\n",
      "Error metric:  119.15180969238281\n",
      "Error metric:  102.28519439697266\n",
      "Error metric:  121.82010650634766\n",
      "Error metric:  106.57142639160156\n",
      "Error metric:  111.75078582763672\n",
      "Error metric:  120.9787368774414\n",
      "Error metric:  112.02426147460938\n",
      "Error metric:  79.8620376586914\n",
      "Error metric:  126.78632354736328\n",
      "Error metric:  118.19127655029297\n",
      "Error metric:  115.7009506225586\n",
      "Error metric:  123.45536041259766\n",
      "Error metric:  125.89810180664062\n",
      "Error metric:  113.58796691894531\n",
      "Error metric:  98.4000015258789\n",
      "Error metric:  128.63970947265625\n",
      "Error metric:  116.75010681152344\n",
      "Error metric:  111.17041015625\n",
      "Error metric:  117.99978637695312\n",
      "Error metric:  114.0633316040039\n",
      "Error metric:  131.88255310058594\n",
      "Error metric:  107.30237579345703\n",
      "Error metric:  125.88310241699219\n",
      "Error metric:  113.59629821777344\n",
      "Error metric:  117.40624237060547\n",
      "Error metric:  121.64146423339844\n",
      "Error metric:  116.9920425415039\n",
      "Error metric:  120.36266326904297\n",
      "Error metric:  108.81736755371094\n",
      "Error metric:  120.02720642089844\n",
      "Error metric:  109.51165771484375\n",
      "Error metric:  123.7824478149414\n",
      "Error metric:  110.59593963623047\n",
      "Error metric:  123.24453735351562\n",
      "Error metric:  116.57923126220703\n",
      "Error metric:  121.2872543334961\n",
      "Error metric:  122.795654296875\n",
      "Error metric:  127.55813598632812\n",
      "Error metric:  126.69688415527344\n",
      "Error metric:  114.44151306152344\n",
      "Error metric:  126.56700897216797\n",
      "Error metric:  111.70848846435547\n",
      "Error metric:  91.66680908203125\n",
      "received EOF from the parser on simplistic , silly and tedious .\n",
      "Error metric:  111.8094482421875\n",
      "Error metric:  126.60091400146484\n",
      "Error metric:  112.7337875366211\n",
      "Error metric:  112.26722717285156\n",
      "Error metric:  106.98738861083984\n",
      "Error metric:  127.25391387939453\n",
      "Error metric:  99.19991302490234\n",
      "Error metric:  127.19270324707031\n",
      "Error metric:  129.93898010253906\n",
      "Error metric:  114.49676513671875\n",
      "Error metric:  100.878173828125\n",
      "Error metric:  54.00000762939453\n",
      "Error metric:  127.91358184814453\n",
      "Error metric:  116.24918365478516\n",
      "Error metric:  119.86347198486328\n",
      "Error metric:  108.10414123535156\n",
      "Error metric:  128.43333435058594\n",
      "Error metric:  121.15824127197266\n",
      "Error metric:  76.64398956298828\n",
      "Error metric:  125.810302734375\n",
      "Error metric:  116.405029296875\n",
      "Error metric:  121.66706848144531\n",
      "Error metric:  116.60797882080078\n",
      "Error metric:  124.99166870117188\n",
      "Error metric:  122.94275665283203\n",
      "Error metric:  116.55630493164062\n",
      "Error metric:  111.3314437866211\n",
      "Error metric:  112.25460815429688\n",
      "Error metric:  90.4000015258789\n",
      "Error metric:  117.1076431274414\n",
      "Error metric:  123.61677551269531\n",
      "Error metric:  108.0\n",
      "Error metric:  113.20294189453125\n",
      "Error metric:  123.4058609008789\n",
      "Error metric:  112.06429290771484\n",
      "Error metric:  81.3565673828125\n",
      "Error metric:  118.78397369384766\n",
      "Error metric:  124.94620513916016\n",
      "Error metric:  118.67121124267578\n",
      "Error metric:  116.83900451660156\n",
      "Error metric:  117.44283294677734\n",
      "Error metric:  109.71772003173828\n",
      "Error metric:  125.059326171875\n",
      "Error metric:  130.39759826660156\n",
      "Error metric:  100.3269271850586\n",
      "Error metric:  128.00527954101562\n",
      "Error metric:  95.34220123291016\n",
      "Error metric:  75.34115600585938\n",
      "Error metric:  125.97974395751953\n",
      "Error metric:  117.35733032226562\n",
      "Error metric:  118.83204650878906\n",
      "Error metric:  132.1601104736328\n",
      "Error metric:  97.99961853027344\n",
      "Error metric:  124.26463317871094\n",
      "Error metric:  119.07655334472656\n",
      "Error metric:  124.46348571777344\n",
      "Error metric:  127.91065979003906\n",
      "Error metric:  118.13236999511719\n",
      "Error metric:  127.52386474609375\n",
      "Error metric:  105.77007293701172\n",
      "Error metric:  114.31291961669922\n",
      "Error metric:  110.00035858154297\n",
      "Error metric:  114.8901596069336\n",
      "Error metric:  122.07929992675781\n",
      "Error metric:  121.20050048828125\n",
      "Error metric:  117.85411834716797\n",
      "Error metric:  122.30767059326172\n",
      "Error metric:  127.42567443847656\n",
      "Error metric:  115.43856048583984\n",
      "Error metric:  113.42891693115234\n",
      "Error metric:  125.07933807373047\n",
      "Error metric:  116.68606567382812\n",
      "Error metric:  127.61333465576172\n",
      "Error metric:  116.94254302978516\n",
      "Error metric:  83.19835662841797\n",
      "Error metric:  120.15684509277344\n",
      "Error metric:  111.07844543457031\n",
      "Error metric:  114.55068969726562\n",
      "Error metric:  73.3309326171875\n",
      "Error metric:  117.985107421875\n",
      "Error metric:  122.98838806152344\n",
      "Error metric:  126.71430969238281\n",
      "Error metric:  106.77867126464844\n",
      "Error metric:  104.9651870727539\n",
      "Error metric:  120.30232238769531\n",
      "Error metric:  118.97361755371094\n",
      "Error metric:  113.51870727539062\n",
      "Error metric:  101.99999237060547\n",
      "Error metric:  114.9869384765625\n",
      "Error metric:  111.4052734375\n",
      "Error metric:  122.05067443847656\n",
      "Error metric:  116.79991149902344\n",
      "Error metric:  122.7423324584961\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "\n",
    "sentim_W = Variable(torch.randn(embedding_size, 1), requires_grad=True)\n",
    "sentim_b = Variable(torch.zeros(1, 1), requires_grad=True)\n",
    "\n",
    "feed_gen = sent_polarity_feed()\n",
    "run_autoencoder(feed_gen, learning_rate,\n",
    "                sentim_classif_fun=get_sent_polarity, sentim_W=sentim_W, sentim_b=sentim_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.030389363722697058\n"
     ]
    }
   ],
   "source": [
    "feed_gen = sent_polarity_feed(is_test=True)\n",
    "run_autoencoder(feed_gen, learning_rate,\n",
    "                sentim_classif_fun=get_sent_polarity, sentim_W=sentim_W, sentim_b=sentim_b,\n",
    "                enable_training=False, measure_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper seems to use just a binary classifier of sentence vectors, without any neural net hidden layers. It is the approach we will try first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Try to do an autoencoder without loops: https://groups.google.com/forum/#!topic/theano-users/O5CM49-jMqQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
