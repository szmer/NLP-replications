{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download the phrase similarity dataset from http://homepages.inf.ed.ac.uk/mlap/resources/index.html, save as `phrase_similarities.txt`\n",
    "2. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'bedewed', 'Chancellorship', 'primatu', 'foreign/non-national', 'Morulae', 'conqueror', 'Super-Bush', 'Â£858', 'B.E.Ward']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497225\n"
     ]
    }
   ],
   "source": [
    "# try stemming just for the embedding?\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in unique_words]\n",
    "stemmed_words = list(set(stemmed_words))\n",
    "print(len(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ID=1\\n(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\\\NP 0 2> (<L (S[dcl]\\\\NP)/NP POS POS chases (S[dcl]\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\\\NP 0 2> (<L (NP\\\\NP)/NP POS POS of (NP\\\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) ) \\n' \n",
      " b'Loading model...\\nModel loaded, ready to parse.\\n'\n"
     ]
    }
   ],
   "source": [
    "# we will run the underlying parser as a subprocess, and intercept its outputs from within Python\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "# .encode() gives bytes instead of str, as .communicate() requires. We get a pair (stdout, stderr):\n",
    "(parse, err) = p.communicate(input='The cat chases a ball of yarn.\\n'.encode())\n",
    "print(parse, '\\n', err)\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T\n",
      "  S[dcl]\n",
      "  1\n",
      "  2>\n",
      "  (<T\n",
      "    NP[nb]\n",
      "    0\n",
      "    2>\n",
      "    (<L NP[nb]/N POS POS The NP[nb]/N>)\n",
      "    (<L N POS POS cat N>))\n",
      "  (<T\n",
      "    S[dcl]\\\\NP\n",
      "    0\n",
      "    2>\n",
      "    (<L (S[dcl]\\\\NP ) /NP POS POS chases (S[dcl]\\\\NP ) /NP>)\n",
      "    (<T\n",
      "      NP[nb]\n",
      "      0\n",
      "      2>\n",
      "      (<T\n",
      "        NP[nb]\n",
      "        0\n",
      "        2>\n",
      "        (<L NP[nb]/N POS POS a NP[nb]/N>)\n",
      "        (<L N POS POS ball N>))\n",
      "      (<T\n",
      "        NP\\\\NP\n",
      "        0\n",
      "        2>\n",
      "        (<L (NP\\\\NP ) /NP POS POS of (NP\\\\NP ) /NP>)\n",
      "        (<T NP 0 1> (<L N POS POS yarn. N>))))))\n"
     ]
    }
   ],
   "source": [
    "# some string cleanup\n",
    "def clean_parse_output(parse_output):\n",
    "    # (remember we have to deal with the parse returned as bytes, not a Unicode string)\n",
    "    lines = str(parse_output).split('\\\\n')\n",
    "    if len(lines) > 1:\n",
    "        return lines[1] # the second line contains the parse itself\n",
    "    else:\n",
    "        return lines[0]\n",
    "\n",
    "from nltk.tree import Tree\n",
    "tree = Tree.fromstring(clean_parse_output(parse))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very pretty, because NLTK decides to print a newline instead of space inside the less/more than signs. In each (parenthesized expression), the first item (head) is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree):\n",
    "    for node in tree:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(<T S[dcl] 1 2> (<T NP 1 2> (<L LRB POS POS \\\\xe2\\\\x80\\\\x98 LRB>) (<T NP 0 1> (<L N POS POS Arrest N>) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<T S[dcl]\\\\\\\\NP 0 2> (<T (S[dcl]\\\\\\\\NP)/PP 0 2> (<L (S[dcl]\\\\\\\\NP)/PP POS POS warrant (S[dcl]\\\\\\\\NP)/PP>) (<L (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) POS POS out (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP)>) ) (<T (S[X]\\\\\\\\NP)\\\\\\\\((S[X]\\\\\\\\NP)/PP) 0 1> (<T PP 0 2> (<L PP/NP POS POS for PP/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Clowes N/N>) (<T N 1 2> (<L N/N POS POS \\\\xe2\\\\x80\\\\x99 N/N>) (<T N 1 2> (<L N/N POS POS partner N/N>) (<L N POS POS years N>) ) ) ) ) ) ) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS before ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS collapse' N>) ) (<L . POS POS . .>) ) ) ) ) \", '(<T NP 1 2> (<L NP/NP POS POS By NP/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Daniel N/N>) (<L N POS POS John N>) ) ) ) ', '(<T S[dcl] 1 2> (<T NP 0 2> (<T NP 0 1> (<L N POS POS AWARRANT N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS for (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS arrest N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS former N/N>) (<L N POS POS partner N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Mr N/N>) (<T N 1 2> (<L N/N POS POS Peter N/N>) (<L N POS POS Clowes N>) ) ) ) ) ) ) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L S[pss]\\\\\\\\NP POS POS issued S[pss]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 1 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS seven N/N>) (<L N POS POS years N>) ) ) (<T ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP 0 2> (<L (((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP)/S[dcl] POS POS before (((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP)/S[dcl]>) (<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS his NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Barlow N/N>) (<T N 1 2> (<L N/N POS POS Clowes N/N>) (<T N 1 2> (<L N/N POS POS investment N/N>) (<L N POS POS empire N>) ) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<T S[dcl]\\\\\\\\NP 0 2> (<L S[dcl]\\\\\\\\NP POS POS collapsed S[dcl]\\\\\\\\NP>) (<L , POS POS , ,>) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/PP POS POS according ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS evidence N>) ) (<T NP\\\\\\\\NP 0 1> (<T S[pss]\\\\\\\\NP 0 2> (<L (S[pss]\\\\\\\\NP)/PP POS POS submitted (S[pss]\\\\\\\\NP)/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Parliamentary N/N>) (<L N POS POS Ombudsman N>) ) ) (<L . POS POS . .>) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ', '(<T S[dcl] 1 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Sir N/N>) (<T N 1 2> (<L N/N POS POS Anthony N/N>) (<L N POS POS Barrowclough N>) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L (S[pss]\\\\\\\\NP)/S[em] POS POS told (S[pss]\\\\\\\\NP)/S[em]>) (<T S[em] 0 2> (<L S[em]/S[dcl] POS POS that S[em]/S[dcl]>) (<T S[dcl] 1 2> (<T NP 0 1> (<L N POS POS police N>) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pt]\\\\\\\\NP) POS POS had (S[dcl]\\\\\\\\NP)/(S[pt]\\\\\\\\NP)>) (<T S[pt]\\\\\\\\NP 0 2> (<L (S[pt]\\\\\\\\NP)/(S[to]\\\\\\\\NP) POS POS wanted (S[pt]\\\\\\\\NP)/(S[to]\\\\\\\\NP)>) (<T S[to]\\\\\\\\NP 0 2> (<L (S[to]\\\\\\\\NP)/(S[b]\\\\\\\\NP) POS POS to (S[to]\\\\\\\\NP)/(S[b]\\\\\\\\NP)>) (<T S[b]\\\\\\\\NP 0 2> (<T S[b]\\\\\\\\NP 0 2> (<L (S[b]\\\\\\\\NP)/NP POS POS interview (S[b]\\\\\\\\NP)/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Mrs N/N>) (<T N 1 2> (<L N/N POS POS Elizabeth N/N>) (<L N POS POS Barlow N>) ) ) ) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS in ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS 1981 N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS over (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS collapse N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<T N 1 2> (<L N/N POS POS stockbroking N/N>) (<L N POS POS firm N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP) POS POS which (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP)>) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/NP POS POS had (S[dcl]\\\\\\\\NP)/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS links N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS with (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS another NP[nb]/N>) (<T N 1 2> (<L N/N POS POS investment N/N>) (<L N POS POS company N>) ) ) (<T NP\\\\\\\\NP 1 2> (<L , POS POS , ,>) (<T NP 0 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Farrington N/N>) (<L N POS POS Stead N>) ) ) (<T NP[nb]\\\\\\\\NP[nb] 1 2> (<L , POS POS , ,>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<T N 1 2> (<L N/N POS POS similar N/N>) (<L N POS POS operation N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS to (NP\\\\\\\\NP)/NP>) (<T NP 0 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Barlow N/N>) (<L N POS POS Clowes N>) ) ) (<L . POS POS . .>) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ', '(<T S[dcl] 1 2> (<T S/S 0 2> (<L (S/S)/PP POS POS According (S/S)/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS liquidator N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS stockbroking N/N>) (<L N POS POS firm N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP) POS POS which (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP)>) (<T S[dcl]\\\\\\\\NP 0 2> (<L S[dcl]\\\\\\\\NP POS POS crashed S[dcl]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS as ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS result N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Farrington N/N>) (<T N 1 2> (<L N/N POS POS Stead N/N>) (<L N POS POS failure N>) ) ) ) (<L , POS POS , ,>) ) ) ) ) ) ) ) ) ) ) ) (<T S[dcl] 1 2> (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS summons N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS for (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<L NP[nb]/N POS POS her NP[nb]/N>) (<L N POS POS arrest N>) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L S[pss]\\\\\\\\NP POS POS issued S[pss]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 1 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP)) POS POS late ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/N POS POS that ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/N>) (<L N POS POS year N>) ) (<L . POS POS . .>) ) ) ) ) ) ) ']\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            sentence = ''\n",
    "            for nested_element in element.iter():\n",
    "                if (nested_element.tag == 'w' or nested_element.tag == 'c') and nested_element.text:\n",
    "                    sentence += ' ' + nested_element.text\n",
    "            parse_output = p.communicate(input=sentence.encode())[0]\n",
    "            p.terminate()\n",
    "            trees.append(clean_parse_output(parse_output))\n",
    "            p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "    bnc_file.close()\n",
    "    if len(trees) >= 3:\n",
    "        break\n",
    "   \n",
    "print(trees[:3])\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = {word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words from the corpus, as their indices in the word map.\n",
    "corpus_words = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_words.append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([545647, 130645, 477349, 423144, 130645, 690827, 636390, 245211,\n",
      "        26147, 153056, 451583, 401296], dtype=int32), array([[697585],\n",
      "       [697585],\n",
      "       [697585],\n",
      "       [697585],\n",
      "       [493937],\n",
      "       [493937],\n",
      "       [493937],\n",
      "       [493937],\n",
      "       [660882],\n",
      "       [660882],\n",
      "       [660882],\n",
      "       [660882]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "def skipgrams_batch(batch_size):\n",
    "    assert batch_size % num_samples == 0\n",
    "    windows_n = batch_size // num_samples\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(windows_n):\n",
    "        target = randint(2, len(corpus_words)-3)\n",
    "        for j in range(num_samples):\n",
    "            labels[i*num_samples+j][0] = corpus_words[target]\n",
    "        batch[i*num_samples] = corpus_words[target-2]\n",
    "        batch[i*num_samples+1] = corpus_words[target-1]\n",
    "        batch[i*num_samples+2] = corpus_words[target+1]\n",
    "        batch[i*num_samples+3] = corpus_words[target+2]\n",
    "        \n",
    "    return batch, labels\n",
    "\n",
    "print(skipgrams_batch(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(unique_words)\n",
    "embedding_size = 70\n",
    "\n",
    "# Model parameters.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The computation graph.\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "\n",
    "# Number of random words to sample apart from the true target; the model should learn to\n",
    "# assign low probability to them given the context.\n",
    "negative_samples_n = 64\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=labels,\n",
    "                                     inputs=embedding_layer,\n",
    "                                     num_sampled=negative_samples_n,\n",
    "                                     num_classes=vocabulary_size))\n",
    "optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.009\n",
      "404.959\n",
      "338.814\n",
      "338.927\n",
      "322.031\n",
      "325.635\n",
      "270.039\n",
      "276.532\n",
      "271.01\n",
      "256.085\n"
     ]
    }
   ],
   "source": [
    "steps_n = 1000\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(steps_n):\n",
    "        batch_inputs, batch_labels = skipgrams_batch(batch_size)\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "        if (i % 100 == 0):\n",
    "            print(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b as l_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_to_encode = []\n",
    "def encoding_error(transf_matrix):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
