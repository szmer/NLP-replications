{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download the phrase similarity dataset from http://homepages.inf.ed.ac.uk/mlap/resources/index.html, save as `phrase_similarities.txt`\n",
    "2. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'windflow', 'Chauliac', 'Fujisankei', 'P(A)', 'race-horses', 'Tigrayans', 'RINK', 'Targeted', 'modicum']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497225\n"
     ]
    }
   ],
   "source": [
    "# try stemming just for the embedding?\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in unique_words]\n",
    "stemmed_words = list(set(stemmed_words))\n",
    "print(len(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ID=1\\n(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\\\NP 0 2> (<L (S[dcl]\\\\NP)/NP POS POS chases (S[dcl]\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\\\NP 0 2> (<L (NP\\\\NP)/NP POS POS of (NP\\\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) ) \\n' \n",
      " b'Loading model...\\nModel loaded, ready to parse.\\n'\n"
     ]
    }
   ],
   "source": [
    "# we will run the underlying parser as a subprocess, and intercept its outputs from within Python\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "# .encode() gives bytes instead of str, as .communicate() requires. We get a pair (stdout, stderr):\n",
    "(parse, err) = p.communicate(input='The cat chases a ball of yarn.\\n'.encode())\n",
    "print(parse, '\\n', err)\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T\n",
      "  S[dcl]\n",
      "  1\n",
      "  2>\n",
      "  (<T\n",
      "    NP[nb]\n",
      "    0\n",
      "    2>\n",
      "    (<L NP[nb]/N POS POS The NP[nb]/N>)\n",
      "    (<L N POS POS cat N>))\n",
      "  (<T\n",
      "    S[dcl]\\\\NP\n",
      "    0\n",
      "    2>\n",
      "    (<L (S[dcl]\\\\NP ) /NP POS POS chases (S[dcl]\\\\NP ) /NP>)\n",
      "    (<T\n",
      "      NP[nb]\n",
      "      0\n",
      "      2>\n",
      "      (<T\n",
      "        NP[nb]\n",
      "        0\n",
      "        2>\n",
      "        (<L NP[nb]/N POS POS a NP[nb]/N>)\n",
      "        (<L N POS POS ball N>))\n",
      "      (<T\n",
      "        NP\\\\NP\n",
      "        0\n",
      "        2>\n",
      "        (<L (NP\\\\NP ) /NP POS POS of (NP\\\\NP ) /NP>)\n",
      "        (<T NP 0 1> (<L N POS POS yarn. N>))))))\n"
     ]
    }
   ],
   "source": [
    "# some string cleanup\n",
    "def clean_parse_output(parse_output):\n",
    "    # (remember we have to deal with the parse returned as bytes, not a Unicode string)\n",
    "    lines = str(parse_output).split('\\\\n')\n",
    "    if len(lines) > 1:\n",
    "        return lines[1] # the second line contains the parse itself\n",
    "    else:\n",
    "        return lines[0]\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parse_output(parse))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very pretty, because NLTK decides to print a newline instead of space inside the less/more than signs. In each (parenthesized expression), the first item (head) is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree):\n",
    "    for node in tree:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(<T S[dcl] 1 2> (<T NP 1 2> (<L LRB POS POS \\\\xe2\\\\x80\\\\x98 LRB>) (<T NP 0 1> (<L N POS POS Arrest N>) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<T S[dcl]\\\\\\\\NP 0 2> (<T (S[dcl]\\\\\\\\NP)/PP 0 2> (<L (S[dcl]\\\\\\\\NP)/PP POS POS warrant (S[dcl]\\\\\\\\NP)/PP>) (<L (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) POS POS out (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP)>) ) (<T (S[X]\\\\\\\\NP)\\\\\\\\((S[X]\\\\\\\\NP)/PP) 0 1> (<T PP 0 2> (<L PP/NP POS POS for PP/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Clowes N/N>) (<T N 1 2> (<L N/N POS POS \\\\xe2\\\\x80\\\\x99 N/N>) (<T N 1 2> (<L N/N POS POS partner N/N>) (<L N POS POS years N>) ) ) ) ) ) ) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS before ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS collapse' N>) ) (<L . POS POS . .>) ) ) ) ) \", '(<T NP 1 2> (<L NP/NP POS POS By NP/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Daniel N/N>) (<L N POS POS John N>) ) ) ) ', '(<T S[dcl] 1 2> (<T NP 0 2> (<T NP 0 1> (<L N POS POS AWARRANT N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS for (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS arrest N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS former N/N>) (<L N POS POS partner N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Mr N/N>) (<T N 1 2> (<L N/N POS POS Peter N/N>) (<L N POS POS Clowes N>) ) ) ) ) ) ) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L S[pss]\\\\\\\\NP POS POS issued S[pss]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 1 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS seven N/N>) (<L N POS POS years N>) ) ) (<T ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP 0 2> (<L (((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP)/S[dcl] POS POS before (((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))\\\\\\\\NP)/S[dcl]>) (<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS his NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Barlow N/N>) (<T N 1 2> (<L N/N POS POS Clowes N/N>) (<T N 1 2> (<L N/N POS POS investment N/N>) (<L N POS POS empire N>) ) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<T S[dcl]\\\\\\\\NP 0 2> (<L S[dcl]\\\\\\\\NP POS POS collapsed S[dcl]\\\\\\\\NP>) (<L , POS POS , ,>) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/PP POS POS according ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS evidence N>) ) (<T NP\\\\\\\\NP 0 1> (<T S[pss]\\\\\\\\NP 0 2> (<L (S[pss]\\\\\\\\NP)/PP POS POS submitted (S[pss]\\\\\\\\NP)/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Parliamentary N/N>) (<L N POS POS Ombudsman N>) ) ) (<L . POS POS . .>) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ', '(<T S[dcl] 1 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Sir N/N>) (<T N 1 2> (<L N/N POS POS Anthony N/N>) (<L N POS POS Barrowclough N>) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L (S[pss]\\\\\\\\NP)/S[em] POS POS told (S[pss]\\\\\\\\NP)/S[em]>) (<T S[em] 0 2> (<L S[em]/S[dcl] POS POS that S[em]/S[dcl]>) (<T S[dcl] 1 2> (<T NP 0 1> (<L N POS POS police N>) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pt]\\\\\\\\NP) POS POS had (S[dcl]\\\\\\\\NP)/(S[pt]\\\\\\\\NP)>) (<T S[pt]\\\\\\\\NP 0 2> (<L (S[pt]\\\\\\\\NP)/(S[to]\\\\\\\\NP) POS POS wanted (S[pt]\\\\\\\\NP)/(S[to]\\\\\\\\NP)>) (<T S[to]\\\\\\\\NP 0 2> (<L (S[to]\\\\\\\\NP)/(S[b]\\\\\\\\NP) POS POS to (S[to]\\\\\\\\NP)/(S[b]\\\\\\\\NP)>) (<T S[b]\\\\\\\\NP 0 2> (<T S[b]\\\\\\\\NP 0 2> (<L (S[b]\\\\\\\\NP)/NP POS POS interview (S[b]\\\\\\\\NP)/NP>) (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Mrs N/N>) (<T N 1 2> (<L N/N POS POS Elizabeth N/N>) (<L N POS POS Barlow N>) ) ) ) ) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS in ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS 1981 N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS over (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS collapse N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<T N 1 2> (<L N/N POS POS stockbroking N/N>) (<L N POS POS firm N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP) POS POS which (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP)>) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/NP POS POS had (S[dcl]\\\\\\\\NP)/NP>) (<T NP 0 2> (<T NP 0 1> (<L N POS POS links N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS with (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS another NP[nb]/N>) (<T N 1 2> (<L N/N POS POS investment N/N>) (<L N POS POS company N>) ) ) (<T NP\\\\\\\\NP 1 2> (<L , POS POS , ,>) (<T NP 0 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Farrington N/N>) (<L N POS POS Stead N>) ) ) (<T NP[nb]\\\\\\\\NP[nb] 1 2> (<L , POS POS , ,>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<T N 1 2> (<L N/N POS POS similar N/N>) (<L N POS POS operation N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS to (NP\\\\\\\\NP)/NP>) (<T NP 0 2> (<T NP 0 1> (<T N 1 2> (<L N/N POS POS Barlow N/N>) (<L N POS POS Clowes N>) ) ) (<L . POS POS . .>) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ', '(<T S[dcl] 1 2> (<T S/S 0 2> (<L (S/S)/PP POS POS According (S/S)/PP>) (<T PP 0 2> (<L PP/NP POS POS to PP/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS liquidator N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS stockbroking N/N>) (<L N POS POS firm N>) ) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP) POS POS which (NP\\\\\\\\NP)/(S[dcl]\\\\\\\\NP)>) (<T S[dcl]\\\\\\\\NP 0 2> (<L S[dcl]\\\\\\\\NP POS POS crashed S[dcl]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP POS POS as ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS result N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS of (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<T N 1 2> (<L N/N POS POS Farrington N/N>) (<T N 1 2> (<L N/N POS POS Stead N/N>) (<L N POS POS failure N>) ) ) ) (<L , POS POS , ,>) ) ) ) ) ) ) ) ) ) ) ) (<T S[dcl] 1 2> (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS the NP[nb]/N>) (<L N POS POS summons N>) ) (<T NP\\\\\\\\NP 0 2> (<L (NP\\\\\\\\NP)/NP POS POS for (NP\\\\\\\\NP)/NP>) (<T NP[nb] 0 2> (<L NP[nb]/N POS POS her NP[nb]/N>) (<L N POS POS arrest N>) ) ) ) (<T S[dcl]\\\\\\\\NP 0 2> (<L (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP) POS POS was (S[dcl]\\\\\\\\NP)/(S[pss]\\\\\\\\NP)>) (<T S[pss]\\\\\\\\NP 0 2> (<L S[pss]\\\\\\\\NP POS POS issued S[pss]\\\\\\\\NP>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 1 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP)) POS POS late ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))>) (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<T (S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP) 0 2> (<L ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/N POS POS that ((S\\\\\\\\NP)\\\\\\\\(S\\\\\\\\NP))/N>) (<L N POS POS year N>) ) (<L . POS POS . .>) ) ) ) ) ) ) ']\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            sentence = ''\n",
    "            for nested_element in element.iter():\n",
    "                if (nested_element.tag == 'w' or nested_element.tag == 'c') and nested_element.text:\n",
    "                    sentence += ' ' + nested_element.text\n",
    "            parse_output = p.communicate(input=sentence.encode())[0]\n",
    "            p.terminate()\n",
    "            trees.append(clean_parse_output(parse_output))\n",
    "            p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "    bnc_file.close()\n",
    "    if len(trees) >= 3:\n",
    "        break\n",
    "   \n",
    "print(trees[:3])\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = {word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words from the corpus, as their indices in the word map.\n",
    "corpus_words = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_words.append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([598819, 501722, 300657, 182194, 501722, 491511, 501722, 491511,\n",
      "       245888,  28561, 405642, 304179], dtype=int32), array([[303895],\n",
      "       [303895],\n",
      "       [303895],\n",
      "       [303895],\n",
      "       [258481],\n",
      "       [258481],\n",
      "       [258481],\n",
      "       [258481],\n",
      "       [265205],\n",
      "       [265205],\n",
      "       [265205],\n",
      "       [265205]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "def skipgrams_batch(batch_size):\n",
    "    assert batch_size % num_samples == 0\n",
    "    windows_n = batch_size // num_samples\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(windows_n):\n",
    "        target = randint(2, len(corpus_words)-3)\n",
    "        for j in range(num_samples):\n",
    "            labels[i*num_samples+j][0] = corpus_words[target]\n",
    "        batch[i*num_samples] = corpus_words[target-2]\n",
    "        batch[i*num_samples+1] = corpus_words[target-1]\n",
    "        batch[i*num_samples+2] = corpus_words[target+1]\n",
    "        batch[i*num_samples+3] = corpus_words[target+2]\n",
    "        \n",
    "    return batch, labels\n",
    "\n",
    "print(skipgrams_batch(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(unique_words)\n",
    "embedding_size = 70\n",
    "\n",
    "# Model parameters.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The computation graph.\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "\n",
    "# Number of random words to sample apart from the true target; the model should learn to\n",
    "# assign low probability to them given the context.\n",
    "negative_samples_n = 64\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=labels,\n",
    "                                     inputs=embedding_layer,\n",
    "                                     num_sampled=negative_samples_n,\n",
    "                                     num_classes=vocabulary_size))\n",
    "optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.083\n",
      "132.346\n",
      "Final loss: 137.598\n",
      "(705241, 70)\n"
     ]
    }
   ],
   "source": [
    "steps_n = 15000 #len(unique_words) * 3\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(steps_n):\n",
    "        batch_inputs, batch_labels = skipgrams_batch(batch_size)\n",
    "        if i+1 == steps_n:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            # TODO meaningful completion info\n",
    "            if (i % 10000 == 0):\n",
    "                print(loss_val)\n",
    "print(trained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.87995267e-02,   7.50405669e-01,  -3.45112562e-01,\n",
       "         4.16200280e-01,   3.97911966e-01,  -8.51840615e-01,\n",
       "         1.10917020e+00,   4.26351130e-01,   9.82077792e-02,\n",
       "        -8.72896552e-01,  -7.05359355e-02,   6.93705916e-01,\n",
       "        -3.22184175e-01,   4.01414007e-01,  -3.21964175e-01,\n",
       "        -2.20627397e-01,  -4.07446831e-01,  -5.31882942e-01,\n",
       "        -5.82467496e-01,   9.64882553e-01,  -9.04482603e-01,\n",
       "        -1.72290668e-01,   1.50034636e-01,  -6.43440843e-01,\n",
       "        -2.64710099e-01,  -8.46308947e-01,  -2.09136419e-02,\n",
       "        -2.10848391e-01,   9.43276763e-01,  -1.55879229e-01,\n",
       "         5.97461402e-01,   8.06708753e-01,   6.64570555e-02,\n",
       "         4.26887095e-01,  -6.75049663e-01,   8.89676213e-01,\n",
       "        -6.56679630e-01,   1.99052677e-01,   8.46626937e-01,\n",
       "        -2.64855206e-01,  -4.92919147e-01,  -8.90498281e-01,\n",
       "        -5.70992649e-01,   6.18981481e-01,   4.09525901e-01,\n",
       "         1.24269322e-01,   5.74584585e-04,   1.13344885e-01,\n",
       "         6.54852509e-01,   6.95104778e-01,  -6.58525050e-01,\n",
       "         5.67925930e-01,   7.42901087e-01,   8.54029655e-01,\n",
       "         8.25590730e-01,   6.78234577e-01,  -8.42285216e-01,\n",
       "        -6.46484256e-01,   8.06618154e-01,  -1.30193695e-01,\n",
       "        -6.52366579e-01,  -4.03005034e-01,  -9.94795680e-01,\n",
       "         6.99968636e-01,  -1.10822415e+00,  -8.20560813e-01,\n",
       "        -5.98067820e-01,  -3.00975025e-01,  -7.48843908e-01,\n",
       "         6.63055360e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_embeddings[word_map['honey'], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b as l_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_matrix = np.random.randn(embedding_size*2, embedding_size)\n",
    "encoding_bias = np.zeros((1, embedding_size))\n",
    "decoding_matrix = np.random.randn(embedding_size, embedding_size*2)\n",
    "decoding_bias = np.zeros((1, embedding_size*2))\n",
    "\n",
    "def encode_node(child1, child2):\n",
    "    \"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size)\"\"\"\n",
    "    return np.tanh(np.dot(np.concatenate((child1, child2)), encoding_matrix) + encoding_bias)\n",
    "\n",
    "def decode_node(node):\n",
    "    return np.tanh(np.dot(node, decoding_matrix) + decoding_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train_batch_size = 5 # number of sentences\n",
    "\n",
    "def encoding_train_batch():\n",
    "    sentence = [] # TODO\n",
    "    \n",
    "    p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "    parse_output = p.communicate(input=' '.join(sentence).encode())[0]\n",
    "    p.terminate()\n",
    "    \n",
    "    # Encode the tree.\n",
    "    tree = ParentedTree.fromstring(clean_parse_output(parse_output))\n",
    "    node_encodings = dict()\n",
    "    for leaf in tree:\n",
    "        node_encodings[leaf] = trained_embeddings[word_map[leaf.label()], ] # TODO\n",
    "    nodes_to_visit = tree.leaves()\n",
    "    while True:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        if not current_node.parent(): # root of the tree\n",
    "            break\n",
    "        left_sibling = current_node.left_sibling()\n",
    "        right_sibling = current_node.right_sibling()\n",
    "        if left_sibling:\n",
    "            node_encodings[current_node.parent()] = encode_node(node_encodings[left_sibling], node_encodings[current_node])\n",
    "        else:\n",
    "            node_encodings[current_node.parent()] = encode_node(node_encodings[current_node], node_encodings[right_sibling])\n",
    "        nodes_to_visit.append(current_node.parent())\n",
    "        \n",
    "    # Decode the tree back again.\n",
    "    nodes_to_visit = [ tree.root() ]\n",
    "    # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "    # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "    node_decodings = dict()\n",
    "    node_decodings[tree.root()] = node_encodings[tree.root()]\n",
    "    encoding_errors = dict()\n",
    "    while True:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        children = [child for child in current_node]\n",
    "        if len(children) > 0: # not a leaf\n",
    "            decoded_node = decode_node(node_decodings[current_node])\n",
    "            encoding_errors[current_node] = node_encodings[current_node] - node_decodings[current_node]\n",
    "            node_decodings[children[0]] = decoded_node[, :embedding_size]\n",
    "            node_decodings[children[1]] = decoded_node[, embedding_size:]\n",
    "            \n",
    "    # Compute the error value and its gradient.\n",
    "    error = reduce(lambda err_sum, node_err: err_sum + node_err, encoding_errors.values(), np.zeros((1, embedding_size*2)))\n",
    "    error = error / len(encoding_errors)\n",
    "    # TODO gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
