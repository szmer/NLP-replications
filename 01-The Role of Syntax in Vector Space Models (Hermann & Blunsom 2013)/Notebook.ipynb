{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing\n",
    "1. Download the phrase similarity dataset from http://homepages.inf.ed.ac.uk/mlap/resources/index.html, save as `phrase_similarities.txt`\n",
    "2. Download the EasyCCG parser from http://homepages.inf.ed.ac.uk/s1049478/easyccg.html, unpack the package (you should get a catalog like `easyccg-0.2`). From the same page, download the regular pretrained model (`model.tar.gz`). Unpack the model to the parser's catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the British National Corpus & the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse BNC XML files with lxml. NLTK technically has a dedicated parser for BNC, which is extremely slow in the lazy mode, and in the non-lazy mode it is very slow and also consumes >8GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_path = 'BNC/Texts/'\n",
    "from os.path import exists\n",
    "\n",
    "def bnc_files_iter():\n",
    "    top_level = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'K']\n",
    "    symbols = top_level + ['L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'V', 'X', 'Y', 'Z',\n",
    "                           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    for top in top_level:\n",
    "        top_path = bnc_path + '/' + top\n",
    "        if not exists(top_path):\n",
    "            continue\n",
    "        for symbol2 in symbols:\n",
    "            path2 = top_path + '/' + top + symbol2\n",
    "            if not exists(path2):\n",
    "                continue\n",
    "            for symbol3 in symbols:\n",
    "                current_path = path2 + '/' + top + symbol2 + symbol3 + '.xml'\n",
    "                if not exists(current_path):\n",
    "                    continue\n",
    "                yield open(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sportiness', '', 'dubiousness', 'sunnily', 'ink-exercise', 'Monteriggioni', 'gastrc', 'womb-water', 'W/100/26/1015', 'hierophants']\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            unique_words.add(element.text.strip())\n",
    "    bnc_file.close()\n",
    "    \n",
    "unique_words = list(unique_words)\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705241\n"
     ]
    }
   ],
   "source": [
    "unique_count = len(unique_words)\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497225\n"
     ]
    }
   ],
   "source": [
    "# try stemming just for the embedding?\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in unique_words]\n",
    "stemmed_words = list(set(stemmed_words))\n",
    "print(len(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CCG parse trees for BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ID=1\\n(<T S[dcl] 1 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS The NP[nb]/N>) (<L N POS POS cat N>) ) (<T S[dcl]\\\\NP 0 2> (<L (S[dcl]\\\\NP)/NP POS POS chases (S[dcl]\\\\NP)/NP>) (<T NP[nb] 0 2> (<T NP[nb] 0 2> (<L NP[nb]/N POS POS a NP[nb]/N>) (<L N POS POS ball N>) ) (<T NP\\\\NP 0 2> (<L (NP\\\\NP)/NP POS POS of (NP\\\\NP)/NP>) (<T NP 0 1> (<L N POS POS yarn. N>) ) ) ) ) ) \\n' \n",
      " b'Loading model...\\nModel loaded, ready to parse.\\n'\n"
     ]
    }
   ],
   "source": [
    "# we will run the underlying parser as a subprocess, and intercept its outputs from within Python\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "# .encode() gives bytes instead of str, as .communicate() requires. We get a pair (stdout, stderr):\n",
    "(parse, err) = p.communicate(input='The cat chases a ball of yarn.\\n'.encode())\n",
    "print(parse, '\\n', err)\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how NLTK can handle parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<T_S[dcl]_1_2>\n",
      "  (<T_NP[nb]_0_2> (The ) (cat ))\n",
      "  (<T_S[dcl]\\NP_0_2>\n",
      "    (chases )\n",
      "    (<T_NP[nb]_0_2>\n",
      "      (<T_NP[nb]_0_2> (a ) (ball ))\n",
      "      (<T_NP\\NP_0_2> (of ) (<T_NP_0_1> (yarn. ))))))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "only_word = re.compile(r'<L\\s\\S+\\sPOS\\sPOS\\s(\\S+)\\s\\S+>')\n",
    "concat_label = re.compile(r'<(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)>')\n",
    "\n",
    "# some string cleanup\n",
    "def clean_parse_output(parse_output):\n",
    "    # (remember we have to deal with the parse returned as bytes, not a Unicode string)\n",
    "    lines = parse_output.decode('utf-8').split('\\n')\n",
    "    parse = ''\n",
    "    if len(lines) > 1:\n",
    "        parse = lines[1] # the second line contains the parse itself\n",
    "    else:\n",
    "        parse = lines[0]\n",
    "    parse = concat_label.sub(lambda match: '<'+match.group(1)+'_'+match.group(2).replace('(', '[').replace(')', ']')\n",
    "                             +'_'+match.group(3)+'_'+match.group(4)+'>',\n",
    "                             only_word.sub(lambda match: match.group(1), parse))\n",
    "    return parse\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "tree = ParentedTree.fromstring(clean_parse_output(parse))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `(parenthesized expression)`, the first item `(head)` is the category of node, and two next items are its child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree):\n",
    "    for node in tree:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-647e8f95ef67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnested_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnested_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mparse_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_parse_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1739\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj2output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m             \u001b[0;31m# All data exchanged.  Translate lists into strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1661\u001b[0m                         \u001b[0;31m# http://bugs.python.org/issue14396.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1663\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exitstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_handle_exitstatus\u001b[0;34m(self, sts, _WIFSIGNALED, _WTERMSIG, _WIFEXITED, _WEXITSTATUS)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_WIFSIGNALED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0m_WTERMSIG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0m_WIFEXITED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WEXITSTATUS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            sentence = ''\n",
    "            for nested_element in element.iter():\n",
    "                if (nested_element.tag == 'w' or nested_element.tag == 'c') and nested_element.text:\n",
    "                    sentence += ' ' + nested_element.text\n",
    "            parse_output = p.communicate(input=sentence.encode())[0]\n",
    "            p.terminate()\n",
    "            trees.append(clean_parse_output(parse_output))\n",
    "            p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "    bnc_file.close()\n",
    "    if len(trees) >= 3:\n",
    "        break\n",
    "   \n",
    "print(trees[:3])\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding procedure will be based on this Tensorflow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistently map each unique word to a integer.\n",
    "word_map = {word: index for index, word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sentences from the corpus, with words as their indices in the word map.\n",
    "corpus_sents = []\n",
    "\n",
    "for bnc_file in bnc_files_iter():\n",
    "    file_tree = etree.parse(bnc_file)\n",
    "    for element in file_tree.iter():\n",
    "        if element.tag == 's':\n",
    "            corpus_sents.append([])\n",
    "        if (element.tag == 'w' or element.tag == 'c') and element.text:\n",
    "            corpus_sents[-1].append(word_map[element.text.strip()])\n",
    "    bnc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of pairs (context word, target word). For simplicity, we hardcode the window size (2) and number of examples in window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([304888, 233474, 648799, 416561, 115724,  20754, 615503, 302849,\n",
      "        24156, 200840, 113164,  55699], dtype=int32), array([[379821],\n",
      "       [379821],\n",
      "       [379821],\n",
      "       [379821],\n",
      "       [442379],\n",
      "       [442379],\n",
      "       [442379],\n",
      "       [442379],\n",
      "       [213717],\n",
      "       [213717],\n",
      "       [213717],\n",
      "       [213717]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "def skipgrams_batch(batch_size):\n",
    "    assert batch_size % num_samples == 0\n",
    "    windows_n = batch_size // num_samples\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(windows_n):\n",
    "        target_sent = randint(0, len(corpus_sents)-1)\n",
    "        while len(corpus_sents[target_sent]) < 5:\n",
    "            target_sent = randint(0, len(corpus_sents)-1)\n",
    "        target = randint(2, len(corpus_sents[target_sent])-3)\n",
    "        for j in range(num_samples):\n",
    "            labels[i*num_samples+j][0] = corpus_sents[target_sent][target]\n",
    "        batch[i*num_samples] = corpus_sents[target_sent][target-2]\n",
    "        batch[i*num_samples+1] = corpus_sents[target_sent][target-1]\n",
    "        batch[i*num_samples+2] = corpus_sents[target_sent][target+1]\n",
    "        batch[i*num_samples+3] = corpus_sents[target_sent][target+2]\n",
    "        \n",
    "    return batch, labels\n",
    "\n",
    "print(skipgrams_batch(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(unique_words)\n",
    "embedding_size = 70\n",
    "\n",
    "# Model parameters.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The computation graph.\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "\n",
    "# Number of random words to sample apart from the true target; the model should learn to\n",
    "# assign low probability to them given the context.\n",
    "negative_samples_n = 64\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=labels,\n",
    "                                     inputs=embedding_layer,\n",
    "                                     num_sampled=negative_samples_n,\n",
    "                                     num_classes=vocabulary_size))\n",
    "optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396.376\n",
      "15.836\n",
      "9.73131\n",
      "11.2018\n",
      "5.75977\n",
      "5.79201\n",
      "3.78162\n",
      "3.32327\n",
      "10.6314\n",
      "3.88557\n",
      "5.53605\n",
      "5.37551\n",
      "3.87442\n",
      "2.33724\n",
      "8.00938\n",
      "1.67723\n",
      "5.31973\n",
      "1.50314\n",
      "1.95304\n",
      "1.49748\n",
      "3.64755\n",
      "4.37122\n",
      "0.300876\n",
      "2.90348\n",
      "0.685659\n",
      "2.57276\n",
      "1.28728\n",
      "2.5585\n",
      "1.25169\n",
      "Final loss: 1.0839\n",
      "(705241, 70)\n"
     ]
    }
   ],
   "source": [
    "steps_n = len(unique_words) * 4\n",
    "trained_embeddings = [] # we want to use them later\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(steps_n):\n",
    "        batch_inputs, batch_labels = skipgrams_batch(batch_size)\n",
    "        if i+1 == steps_n:\n",
    "            _, loss_val, trained_embeddings = sess.run([optimizer, loss, embeddings], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            # TODO meaningful completion info\n",
    "            if (i % 100000 == 0):\n",
    "                print(loss_val)\n",
    "print(trained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8765161 ,  0.57190591,  1.14589   ,  1.88508892, -1.5483526 ,\n",
       "        1.51270044,  1.29161346,  0.75599438,  0.84486055,  1.54948008,\n",
       "        1.92075813, -1.3215512 , -0.58348435,  1.9025389 ,  0.55134082,\n",
       "       -1.90189433,  1.53919566, -1.0111059 ,  0.41306138,  1.42498946,\n",
       "        2.08634377,  1.70511055, -1.52415645, -0.3386088 ,  0.3850292 ,\n",
       "        1.91870797,  2.03005552, -1.43424416, -0.80985343,  1.34230816,\n",
       "       -1.71646845,  0.41091928,  1.55531728,  1.82881594,  1.46796107,\n",
       "       -2.05305195, -1.19118345, -0.24090868, -1.97027183, -1.65812576,\n",
       "       -2.19296861,  1.74225688, -0.57798874, -1.58367789,  1.09224331,\n",
       "        0.31635639, -2.11186266, -1.70546985, -1.33731949,  1.00421917,\n",
       "       -1.50437617, -2.06367993, -1.35685956,  0.47820225,  1.70919514,\n",
       "       -1.72203338, -0.53100717,  1.14944351,  1.59701717,  0.73824239,\n",
       "       -1.13850331,  1.06597984, -1.55652785, -0.36274669, -1.49337304,\n",
       "       -1.21382844, -2.13838172, -1.89023709, -0.71068662, -1.13533103], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_embeddings[word_map['honey'], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b as l_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding_matrix = np.random.randn(embedding_size*2, embedding_size)\n",
    "#encoding_bias = np.zeros((1, embedding_size))\n",
    "#decoding_matrix = np.random.randn(embedding_size, embedding_size*2)\n",
    "#decoding_bias = np.zeros((1, embedding_size*2))\n",
    "init_enc_theta = np.random.randn(4, embedding_size, embedding_size*2)\n",
    "enc_theta_shape = (4, embedding_size, embedding_size*2)\n",
    "\n",
    "# Here the parameters need to passed as function arguments, because they will be optimized\n",
    "# by the LBFGS implementation.\n",
    "def encode_node(child1, child2, enc_theta):\n",
    "    #\"\"\"Both child1 and child2 are numpy arrays of shape (1, embedding_size). Return the encoding\n",
    "    #(1, embedding_size) and partial derivatives for enc_mat and enc_bias.\"\"\"\n",
    "    conc_embeds = np.concatenate((child1, child2))\n",
    "    linear = np.dot(conc_embeds, np.transpose(enc_theta[0, :, :])) + enc_theta[1, :, 0] # enc_mat, enc_bias\n",
    "    d_tanh = 1 - np.tanh(linear) ** 2\n",
    "    d_mat = np.dot(np.reshape(conc_embeds, (conc_embeds.shape[0], 1)), np.reshape(d_tanh, (1, d_tanh.shape[0])))\n",
    "    return np.tanh(linear), d_mat, d_tanh\n",
    "\n",
    "def decode_node(node, enc_theta):\n",
    "    # node is (1, embedding_size), output is (1, 2*embedding_size)\n",
    "    linear = np.dot(node, enc_theta[2, :, :]) + enc_theta[3, 0, :] # dec_mat, dec_bias\n",
    "    d_tanh = 1 - np.tanh(linear) ** 2\n",
    "    d_mat = np.dot(np.reshape(node, (node.shape[0], 1)), np.reshape(d_tanh, (1, d_tanh.shape[0])))\n",
    "    return np.tanh(linear), d_mat, d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "encoding_train_batch_size = 5 # number of sentences\n",
    "\n",
    "batch_d_enc_mat = np.zeros((embedding_size*2, embedding_size))\n",
    "batch_d_enc_bias = np.zeros((1, embedding_size))\n",
    "batch_d_dec_mat = np.zeros((embedding_size, embedding_size*2))\n",
    "batch_d_dec_bias = np.zeros((1, embedding_size*2))\n",
    "def encoding_train_batch(enc_theta):\n",
    "    used_sents = [] # at least don't repeat them in one batch\n",
    "    batch_error = np.zeros((1, embedding_size))\n",
    "    # Partial derivatives -- to be stacked into a gradient at the end.\n",
    "    global batch_d_enc_mat, batch_d_enc_bias, batch_d_dec_mat, batch_d_dec_bias\n",
    "    batch_d_enc_mat = np.zeros((embedding_size*2, embedding_size))\n",
    "    batch_d_enc_bias = np.zeros((1, embedding_size))\n",
    "    batch_d_dec_mat = np.zeros((embedding_size, embedding_size*2))\n",
    "    batch_d_dec_bias = np.zeros((1, embedding_size*2))\n",
    "    # this is used to scale down the derivatives, ie. averaging\n",
    "    nodes_n = 0\n",
    "    enc_theta = np.reshape(enc_theta, enc_theta_shape)\n",
    "    print(enc_theta.shape)\n",
    "    \n",
    "    for i in range(encoding_train_batch_size):\n",
    "        sentence_n = randint(0, len(corpus_sents))\n",
    "        while sentence_n in used_sents:\n",
    "            sentence_n = randint(0, len(corpus_sents))\n",
    "        sentence = corpus_sents[sentence_n]\n",
    "        used_sents.append(sentence_n)\n",
    "        \n",
    "        p = Popen(['java', '-jar', 'easyccg-0.2/easyccg.jar', '--model', 'easyccg-0.2/model'], stdout=PIPE, stdin=PIPE, stderr=PIPE)\n",
    "        #print([unique_words[word_id] for word_id in sentence])\n",
    "        parse_output = p.communicate(input=' '.join([unique_words[word_id] for word_id in sentence]).encode())[0]\n",
    "        p.terminate()\n",
    "        \n",
    "        # Collect partial derivatives for this sentence; in fact later they will have to be chained\n",
    "        # when we will know the error (becaus it uses absolute value).\n",
    "        #sent_d_enc_mat = {}\n",
    "        #sent_d_enc_bias = {}\n",
    "        \n",
    "        # Handle special treatment of parens by our parser.\n",
    "        def nd_lbl(node):\n",
    "            if node.label() == '-LRB-':\n",
    "                return '('\n",
    "            elif node.label() == '-RRB-':\n",
    "                return ')'\n",
    "            else:\n",
    "                return node.label()\n",
    "        \n",
    "        # Encode the tree.\n",
    "        node_encodings = dict()\n",
    "        #print(clean_parse_output(parse_output))\n",
    "        tree = ParentedTree.fromstring(clean_parse_output(parse_output))\n",
    "        def encode_tree(node):\n",
    "            subtrees = [subtr for subtr in node]\n",
    "            if len(subtrees) == 0: # a leaf\n",
    "                node_encodings[nd_lbl(node)] = trained_embeddings[word_map[nd_lbl(node)], ]\n",
    "            elif len(subtrees) == 1:\n",
    "                encode_tree(subtrees[0])\n",
    "                node_encodings[nd_lbl(node)] = node_encodings[nd_lbl(subtrees[0])]\n",
    "            else:\n",
    "                if len(subtrees) != 2: # dbg\n",
    "                    print(subtrees)\n",
    "                encode_tree(subtrees[0])\n",
    "                encode_tree(subtrees[1])\n",
    "                node_encodings[nd_lbl(node)], d_enc_mat, d_enc_bias = encode_node(\n",
    "                    node_encodings[nd_lbl(subtrees[0])],\n",
    "                    node_encodings[nd_lbl(subtrees[1])],\n",
    "                    enc_theta)\n",
    "                global batch_d_enc_mat, batch_d_enc_bias \n",
    "                batch_d_enc_mat = batch_d_enc_mat + d_enc_mat\n",
    "                batch_d_enc_bias = batch_d_enc_bias + d_enc_bias\n",
    "        encode_tree(tree)\n",
    "        \n",
    "        # Decode the tree back again.\n",
    "        # this dictionary in fact maps nodes to their *partial* decodings from which their children are to be\n",
    "        # recreated; thus for the root it's just its encoding, from which we will retrieve immediate children\n",
    "        node_decodings = dict()\n",
    "        node_decodings[nd_lbl(tree.root())] = node_encodings[nd_lbl(tree.root())]\n",
    "        encoding_errors = dict()\n",
    "        nodes_to_visit = [ tree.root() ]\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            children = [child for child in current_node]\n",
    "            if len(children) > 0: # not a leaf\n",
    "                decoded_node, d_dec_mat, d_dec_bias = decode_node(node_decodings[nd_lbl(current_node)],\n",
    "                                                                  enc_theta)\n",
    "                node_decodings[nd_lbl(children[0])] = decoded_node[:embedding_size]\n",
    "                node_decodings[nd_lbl(children[1])] = decoded_node[embedding_size:]\n",
    "                # Get the error and partial derivatives (both (1, 2*embedding_size)).\n",
    "                encoding_errors[nd_lbl(current_node)] = (node_encodings[nd_lbl(current_node)]\n",
    "                                                         - node_decodings[nd_lbl(current_node)])\n",
    "                batch_d_dec_mat = batch_d_dec_mat - d_dec_mat # dec is the minuend, so its part dev is -1\n",
    "                batch_d_dec_bias = batch_d_dec_bias - d_dec_bias\n",
    "                # np.abs()\n",
    "                #d_encoding = ((node_encodings[current_node] - node_decodings[current_node])\n",
    "                #              / encoding_errors[current_node])\n",
    "                #d_decoding = ((node_decodings[current_node] - node_encodings[current_node])\n",
    "                #              / encoding_errors[current_node])\n",
    "                #batch_d_enc_mat = batch_d_enc_mat + np.dot(d_encoding, sent_d_enc_mat[current_node])\n",
    "                #batch_d_enc_bias = batch_d_enc_bias + np.dot(d_encoding, sent_d_enc_bias[current_node])\n",
    "                nodes_n += 1\n",
    "        \n",
    "        # Compute the error value.\n",
    "        sent_error = reduce(lambda err_sum, node_err: err_sum + node_err, encoding_errors.values(), np.zeros((1, embedding_size)))\n",
    "        sent_error = sent_error / len(encoding_errors)\n",
    "        # Update batch error.\n",
    "        batch_error = batch_error + sent_error / encoding_train_batch_size\n",
    "\n",
    "    # TODO regularization    \n",
    "    batch_gradient = np.zeros((4, embedding_size, embedding_size*2))\n",
    "    batch_gradient[0, :, :] = np.transpose(batch_d_enc_mat) / nodes_n\n",
    "    batch_gradient[1, :, 0] = batch_d_enc_bias / nodes_n\n",
    "    batch_gradient[2, :, :] = batch_d_dec_mat / nodes_n\n",
    "    batch_gradient[3, 0, :] = batch_d_dec_bias / nodes_n\n",
    "    #[x / nodes_n for x in [batch_d_enc_mat, batch_d_enc_bias,\n",
    "    #                                        batch_d_dec_mat, batch_d_dec_bias]]\n",
    "    return batch_error, np.reshape(batch_gradient, (batch_gradient.size,)) # we need to return a 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n",
      "(4, 70, 140)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.22748692, -0.95984159,  0.25619557, ...,  0.30072599,\n",
       "        -0.48800971,  2.42655409]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.]]),\n",
       " {'funcalls': 9,\n",
       "  'grad': array([ 0.55682554,  0.95166548,  1.62743453, ...,  0.        ,\n",
       "          0.        ,  0.        ]),\n",
       "  'nit': 1,\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_bfgs(encoding_train_batch, init_enc_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
